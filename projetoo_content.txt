Project Structure:

├── Memoraith/
│   ├── app.py
│   ├── problems.py
│   ├── setup.py
│   ├── testdata.py
│   ├── .git/
│   │   ├── hooks/
│   │   ├── info/
│   │   ├── logs/
│   │   │   ├── refs/
│   │   │   │   ├── heads/
│   │   │   │   ├── remotes/
│   │   │   │   │   ├── origin/
│   │   ├── objects/
│   │   │   ├── 01/
│   │   │   ├── 02/
│   │   │   ├── 04/
│   │   │   ├── 06/
│   │   │   ├── 08/
│   │   │   ├── 09/
│   │   │   ├── 0a/
│   │   │   ├── 0b/
│   │   │   ├── 0d/
│   │   │   ├── 0e/
│   │   │   ├── 13/
│   │   │   ├── 15/
│   │   │   ├── 17/
│   │   │   ├── 19/
│   │   │   ├── 1b/
│   │   │   ├── 1f/
│   │   │   ├── 20/
│   │   │   ├── 25/
│   │   │   ├── 28/
│   │   │   ├── 2b/
│   │   │   ├── 30/
│   │   │   ├── 34/
│   │   │   ├── 35/
│   │   │   ├── 37/
│   │   │   ├── 39/
│   │   │   ├── 3b/
│   │   │   ├── 41/
│   │   │   ├── 43/
│   │   │   ├── 47/
│   │   │   ├── 49/
│   │   │   ├── 4a/
│   │   │   ├── 4b/
│   │   │   ├── 4e/
│   │   │   ├── 4f/
│   │   │   ├── 50/
│   │   │   ├── 53/
│   │   │   ├── 59/
│   │   │   ├── 5d/
│   │   │   ├── 5f/
│   │   │   ├── 60/
│   │   │   ├── 62/
│   │   │   ├── 66/
│   │   │   ├── 67/
│   │   │   ├── 6b/
│   │   │   ├── 6d/
│   │   │   ├── 6e/
│   │   │   ├── 70/
│   │   │   ├── 71/
│   │   │   ├── 73/
│   │   │   ├── 74/
│   │   │   ├── 77/
│   │   │   ├── 78/
│   │   │   ├── 7a/
│   │   │   ├── 7b/
│   │   │   ├── 7c/
│   │   │   ├── 7d/
│   │   │   ├── 7e/
│   │   │   ├── 80/
│   │   │   ├── 84/
│   │   │   ├── 85/
│   │   │   ├── 89/
│   │   │   ├── 8c/
│   │   │   ├── 8d/
│   │   │   ├── 8e/
│   │   │   ├── 90/
│   │   │   ├── 93/
│   │   │   ├── 96/
│   │   │   ├── 98/
│   │   │   ├── 9f/
│   │   │   ├── a0/
│   │   │   ├── a3/
│   │   │   ├── a4/
│   │   │   ├── a6/
│   │   │   ├── a8/
│   │   │   ├── a9/
│   │   │   ├── ad/
│   │   │   ├── b2/
│   │   │   ├── b4/
│   │   │   ├── b7/
│   │   │   ├── b9/
│   │   │   ├── bc/
│   │   │   ├── bf/
│   │   │   ├── c0/
│   │   │   ├── c4/
│   │   │   ├── c7/
│   │   │   ├── c8/
│   │   │   ├── c9/
│   │   │   ├── cd/
│   │   │   ├── cf/
│   │   │   ├── d0/
│   │   │   ├── d7/
│   │   │   ├── d9/
│   │   │   ├── da/
│   │   │   ├── dc/
│   │   │   ├── de/
│   │   │   ├── e1/
│   │   │   ├── e2/
│   │   │   ├── e6/
│   │   │   ├── e7/
│   │   │   ├── e8/
│   │   │   ├── e9/
│   │   │   ├── f0/
│   │   │   ├── f1/
│   │   │   ├── f3/
│   │   │   ├── f5/
│   │   │   ├── f7/
│   │   │   ├── f8/
│   │   │   ├── fa/
│   │   │   ├── fc/
│   │   │   ├── ff/
│   │   │   ├── info/
│   │   │   ├── pack/
│   │   ├── refs/
│   │   │   ├── heads/
│   │   │   ├── remotes/
│   │   │   │   ├── origin/
│   │   │   ├── tags/
│   ├── .github/
│   │   ├── workflows/
│   ├── .idea/
│   │   ├── inspectionProfiles/
│   ├── .pytest_cache/
│   │   ├── v/
│   │   │   ├── cache/
│   ├── assets/
│   ├── examples/
│   │   ├── example_pytorch.py
│   │   ├── example_tensorflow.py
│   ├── memoraith/
│   │   ├── cli.py
│   │   ├── config.py
│   │   ├── exceptions.py
│   │   ├── logging_config.py
│   │   ├── profiler.py
│   │   ├── __init__.py
│   │   ├── analysis/
│   │   │   ├── analyzer.py
│   │   │   ├── anomaly_detection.py
│   │   │   ├── bottleneck.py
│   │   │   ├── metrics.py
│   │   │   ├── recommendations.py
│   │   │   ├── __init__.py
│   │   │   ├── __pycache__/
│   │   ├── data_collection/
│   │   │   ├── base_collector.py
│   │   │   ├── cpu_memory.py
│   │   │   ├── gpu_memory.py
│   │   │   ├── network_memory_system.py
│   │   │   ├── resource_lock.py
│   │   │   ├── time_tracking.py
│   │   │   ├── __init__.py
│   │   │   ├── __pycache__/
│   │   ├── integration/
│   │   │   ├── common_utils.py
│   │   │   ├── framework_adapter.py
│   │   │   ├── pytorch_adapter.py
│   │   │   ├── tensorflow_adapter.py
│   │   │   ├── __init__.py
│   │   │   ├── __pycache__/
│   │   ├── reporting/
│   │   │   ├── console_report.py
│   │   │   ├── export_utils.py
│   │   │   ├── report_generator.py
│   │   │   ├── __init__.py
│   │   │   ├── __pycache__/
│   │   ├── templates/
│   │   ├── visualization/
│   │   │   ├── heatmap.py
│   │   │   ├── interactive_dashboard.py
│   │   │   ├── plot_memory.py
│   │   │   ├── plot_time.py
│   │   │   ├── real_time_visualizer.py
│   │   │   ├── __init__.py
│   │   │   ├── __pycache__/
│   │   ├── __pycache__/
│   ├── memoraith_reports/
│   │   ├── memoraith_exports/
│   ├── __pycache__/


================================================================================

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\app.py

import asyncio
import logging
from pathlib import Path
from typing import Optional, Dict, Any
from .config import config
from .exceptions import MemoraithError
from .profiler import profile_model, set_output_path

class Memoraith:
    """Main application class for Memoraith profiler."""

    def __init__(self, config_file: Optional[str] = None):
        self.logger = logging.getLogger(__name__)
        if config_file:
            config.load_from_file(config_file)

    async def setup(self) -> None:
        """Initialize the profiler setup."""
        try:
            # Ensure output directory exists
            config.output_path.mkdir(parents=True, exist_ok=True)

            # Validate configuration
            if not config.validate():
                raise MemoraithError("Invalid configuration")

            self.logger.info("Memoraith setup completed successfully")
        except Exception as e:
            self.logger.error(f"Setup failed: {str(e)}")
            raise

    @profile_model()
    async def profile(self, model: Any, *args: Any, **kwargs: Any) -> Dict[str, Any]:
        """Profile a model with the current configuration."""
        try:
            return await self._run_profiling(model, *args, **kwargs)
        except Exception as e:
            self.logger.error(f"Profiling failed: {str(e)}")
            raise

    async def _run_profiling(self, model: Any, *args: Any, **kwargs: Any) -> Dict[str, Any]:
        """Internal method to run the profiling process."""
        try:
            # Model profiling is handled by the @profile_model decorator
            result = await model(*args, **kwargs)
            return result
        except Exception as e:
            self.logger.error(f"Error during profiling execution: {str(e)}")
            raise

    async def cleanup(self) -> None:
        """Cleanup resources after profiling."""
        try:
            # Add any cleanup logic here
            self.logger.info("Cleanup completed successfully")
        except Exception as e:
            self.logger.error(f"Cleanup failed: {str(e)}")
            raise

async def main() -> None:
    """Main entry point for the application."""
    try:
        memoraith = Memoraith()
        await memoraith.setup()
        # Example usage would go here
        await memoraith.cleanup()
    except Exception as e:
        logging.error(f"Application error: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())
Class: Memoraith
--------------------------------------------------------------------------------
  Method: __init__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\problems.py

import os
import subprocess
import json
from pathlib import Path

def run_pylint(project_dir):
    """
    Runs pylint on the specified project directory and returns the JSON output.
    """
    try:
        # Run pylint with JSON output
        result = subprocess.run(
            ['pylint', project_dir, '--output-format=json'],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            check=False  # Don't raise exception on non-zero exit
        )

        if result.stderr:
            print("Pylint encountered an error:")
            print(result.stderr)
            # Continue processing even if pylint reports errors (like syntax errors)

        # Parse JSON output
        pylint_output = json.loads(result.stdout)
        return pylint_output

    except FileNotFoundError:
        print("Pylint is not installed or not found in the system PATH.")
        return None
    except json.JSONDecodeError:
        print("Failed to parse pylint output. Ensure pylint is producing valid JSON.")
        return None

def extract_errors(pylint_output):
    """
    Extracts only error and fatal issues from pylint output.

    Args:
        pylint_output (list): The JSON-parsed output from pylint.

    Returns:
        list: Filtered list of error issues.
    """
    error_issues = [
        {
            'File': issue.get('path', ''),
            'Line': issue.get('line', ''),
            'Column': issue.get('column', ''),
            'Symbol': issue.get('symbol', ''),
            'Message': issue.get('message', ''),
            'Type': issue.get('type', '')
        }
        for issue in pylint_output
        if issue.get('type', '').lower() in ['error', 'fatal'] and issue.get('message-id', '').startswith(('E', 'F'))
    ]

    return error_issues

def main():
    # Define your project directory
    project_dir = Path(r'C:\Users\PC\Desktop\Leo-Major\Memoraith')

    if not project_dir.exists():
        print(f"The directory {project_dir} does not exist.")
        return

    print(f"Running pylint on {project_dir}...")

    pylint_output = run_pylint(str(project_dir))

    if pylint_output is None:
        print("No pylint output to process.")
        return

    relevant_errors = extract_errors(pylint_output)

    print("\n=== Pylint Errors ===")
    if relevant_errors:
        for issue in relevant_errors:
            print(f"{issue['File']}:{issue['Line']}:{issue['Column']} - {issue['Message']} [{issue['Symbol']}] ({issue['Type'].capitalize()})")
    else:
        print("No errors found.")

    # Optionally, save the results to a file
    save_results = True  # Set to False if you don't want to save
    if save_results:
        errors_file = project_dir / 'pylint_errors.txt'

        with open(errors_file, 'w', encoding='utf-8') as f:
            for issue in relevant_errors:
                f.write(f"{issue['File']}:{issue['Line']}:{issue['Column']} - {issue['Message']} [{issue['Symbol']}] ({issue['Type'].capitalize()})\n")

        print(f"\nErrors saved to {errors_file}")

if __name__ == "__main__":
    main()


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\setup.py

from setuptools import setup, find_packages

setup(
    name="memoraith",
    version="0.4.0",  # Updated version number
    author="Mehdi El Jouhfi",
    author_email="midojouhfi@gmail.com",
    description="Advanced lightweight model profiler for deep learning frameworks",
    long_description=open("README.md").read(),
    long_description_content_type="text/markdown",
    url="https://github.com/mehdi342/Memoraith",
    packages=find_packages(exclude=['tests', 'tests.*']),
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.7",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
    ],
    python_requires='>=3.7',
    install_requires=[
        'torch>=1.7.0',
        'tensorflow>=2.4.0',
        'matplotlib>=3.3.0',
        'seaborn>=0.11.0',
        'plotly>=4.14.0',
        'pandas>=1.2.0',
        'jinja2>=2.11.0',
        'pdfkit>=0.6.0',
        'psutil>=5.8.0',
        'pynvml>=8.0.0',
        'colorama>=0.4.4',
        'tqdm>=4.60.0',
        'aiofiles>=0.6.0',
        'asyncio>=3.4.3',
        'networkx>=2.5',
        'python-dotenv>=0.19.0',
        'pyyaml>=5.4.0'
    ],
    extras_require={
        'full': [
            'torch>=1.7.0',
            'tensorflow>=2.4.0',
            'tensorboard>=2.4.0',
            'optuna>=2.3.0',
            'ray>=1.2.0',
        ],
        'dev': [
            'pytest>=6.2.0',
            'pytest-asyncio>=0.14.0',
            'black>=20.8b1',
            'isort>=5.7.0',
            'flake8>=3.8.0',
            'mypy>=0.800',
            'tox>=3.20.0',
            'sphinx>=3.4.3',
            'sphinx-rtd-theme>=0.5.1',
        ],
    },
    entry_points={
        'console_scripts': [
            'memoraith=memoraith.cli:main',
        ],
    },
    include_package_data=True,
    zip_safe=False,
    project_urls={
        'Bug Reports': 'https://github.com/mehdi342/Memoraith/issues',
        'Source': 'https://github.com/mehdi342/Memoraith/',
    },
)

--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\testdata.py

import os
import re

def generate_project_structure(directory, indent_level=0):
    structure = ""
    for root, dirs, files in os.walk(directory):
        if 'venv' in root:
            continue

        level = root.replace(directory, '').count(os.sep)
        indent = '│   ' * (level - indent_level)
        structure += f"{indent}├── {os.path.basename(root)}/\n"
        sub_indent = '│   ' * (level + 1 - indent_level)
        for file in files:
            if file.endswith('.py'):
                structure += f"{sub_indent}├── {file}\n"
        dirs[:] = [d for d in dirs if d != 'venv']  # Skip venv directory

    return structure

def extract_classes_and_methods(content):
    class_regex = r'class\s+(\w+)\s*(\(.*?\))?:'
    method_regex = r'def\s+(\w+)\s*\(.*?\):'

    extracted_content = ""
    class_matches = re.findall(class_regex, content)

    for class_match in class_matches:
        class_name = class_match[0]
        extracted_content += f"\nClass: {class_name}\n"
        extracted_content += "-" * 80 + "\n"

        method_matches = re.findall(method_regex, content)
        for method_match in method_matches:
            extracted_content += f"  Method: {method_match}\n"

    return extracted_content

def read_files_recursively(directory):
    content = ""
    for root, dirs, files in os.walk(directory):
        if 'venv' in root:
            continue

        for file in files:
            if file.endswith('.py'):
                file_path = os.path.join(root, file)
                print(f"Processing file: {file_path}")
                content += f"File: {file_path}\n\n"
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        file_content = f.read()
                        content += file_content

                        extracted_classes_methods = extract_classes_and_methods(file_content)
                        content += extracted_classes_methods

                except UnicodeDecodeError:
                    try:
                        with open(file_path, 'r', encoding='ISO-8859-1') as f:
                            file_content = f.read()
                            content += file_content
                    except Exception as e:
                        content += f"Error reading file: {e}"
                content += "\n\n" + "-"*80 + "\n\n"
    return content

def save_content_to_txt(directory, output_file):
    print("Starting the process...")
    project_structure = generate_project_structure(directory)
    file_content = read_files_recursively(directory)
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("Project Structure:\n\n")
        f.write(project_structure)
        f.write("\n\n" + "="*80 + "\n\n")
        f.write(file_content)
    print("Process completed successfully.")

# Usage
project_directory = r"C:\Users\PC\Desktop\Leo-Major\Memoraith"
output_file = r"C:\Users\PC\Desktop\Leo-Major\Memoraith\projetoo_content.txt"

try:
    save_content_to_txt(project_directory, output_file)
except PermissionError:
    print("Permission denied. Please check your write permissions or choose a different output location.")
except Exception as e:
    print(f"An error occurred: {e}")


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\examples\example_pytorch.py

# PyTorch example


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\examples\example_tensorflow.py

import tensorflow as tf
from memoraith import profile_model, set_output_path

def create_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(1)
    ])
    return model

@profile_model(memory=True, computation=True, gpu=True)
def train_model(model, epochs=5):
    model.compile(optimizer='adam', loss='mse')

    # Generate some dummy data
    x_train = tf.random.normal((1000, 10))
    y_train = tf.random.normal((1000, 1))

    model.fit(x_train, y_train, epochs=epochs, verbose=1)

if __name__ == "__main__":
    set_output_path('tensorflow_profiling_results/')
    model = create_model()
    train_model(model)

--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\cli.py

import argparse
import asyncio
from typing import Any
from memoraith import profile_model, set_output_path
from memoraith.config import config
from memoraith.exceptions import MemoraithError

async def main() -> None:
    parser = argparse.ArgumentParser(description="Memoraith: Lightweight Model Profiler")
    parser.add_argument("module", help="Python module containing the model and training function")
    parser.add_argument("function", help="Name of the function to profile")
    parser.add_argument("--output", default="memoraith_reports", help="Output directory for profiling results")
    parser.add_argument("--memory", action="store_true", help="Enable memory profiling")
    parser.add_argument("--computation", action="store_true", help="Enable computation time profiling")
    parser.add_argument("--gpu", action="store_true", help="Enable GPU profiling")
    parser.add_argument("--real-time", action="store_true", help="Enable real-time visualization")
    parser.add_argument("--config", help="Path to configuration file")
    parser.add_argument("--report-format", choices=['html', 'pdf'], default='html', help="Report format")

    args = parser.parse_args()

    try:
        if args.config:
            config.load_from_file(args.config)

        set_output_path(args.output)

        module = __import__(args.module)
        func = getattr(module, args.function)

        @profile_model(memory=args.memory, computation=args.computation, gpu=args.gpu,
                       real_time_viz=args.real_time, report_format=args.report_format)
        async def wrapped_func(*args: Any, **kwargs: Any) -> Any:
            if asyncio.iscoroutinefunction(func):
                return await func(*args, **kwargs)
            else:
                return await asyncio.to_thread(func, *args, **kwargs)

        await wrapped_func()

    except ImportError as e:
        print(f"Error: Could not import module '{args.module}'. {str(e)}")
    except AttributeError as e:
        print(f"Error: Function '{args.function}' not found in module '{args.module}'. {str(e)}")
    except MemoraithError as e:
        print(f"Memoraith Error: {str(e)}")
    except Exception as e:
        print(f"An unexpected error occurred: {str(e)}")

if __name__ == "__main__":
    asyncio.run(main())

--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\config.py

# config.py
import os
import logging
from pathlib import Path
from typing import Dict, Any, Optional
from dotenv import load_dotenv
import yaml
import json
import torch.optim as optim
import torch.nn as nn

class Config:
    """Finnaly a good config ."""

    def __init__(self):
        # Core settings
        self.output_path = Path('memoraith_reports/')
        self.enable_gpu = False
        self.enable_memory = True
        self.enable_time = True
        self.report_format = 'html'
        self.real_time_viz = False

        # Profiling settings
        self.profiling_interval = 0.1
        self.max_memory_samples = 1000
        self.bottleneck_threshold = 0.1
        self.anomaly_threshold = 3.0

        # Training settings
        self.batch_size = 32
        self.max_epochs = 100
        self.learning_rate = 0.001
        self.optimizer = 'adam'
        self.loss_function = 'cross_entropy'

        # Logging settings
        self.log_level = logging.INFO
        self.enable_console_output = True
        self.enable_file_logging = True

        # Load environment variables
        load_dotenv()
        self.load_from_env()

    def load_from_env(self) -> None:
        """Load configuration from environment variables."""
        self.output_path = Path(os.getenv('MEMORAITH_OUTPUT_PATH', str(self.output_path)))
        self.enable_gpu = os.getenv('MEMORAITH_ENABLE_GPU', str(self.enable_gpu)).lower() == 'true'
        self.enable_memory = os.getenv('MEMORAITH_ENABLE_MEMORY', str(self.enable_memory)).lower() == 'true'
        self.enable_time = os.getenv('MEMORAITH_ENABLE_TIME', str(self.enable_time)).lower() == 'true'
        self.batch_size = int(os.getenv('MEMORAITH_BATCH_SIZE', str(self.batch_size)))
        self.max_epochs = int(os.getenv('MEMORAITH_MAX_EPOCHS', str(self.max_epochs)))
        self.learning_rate = float(os.getenv('MEMORAITH_LEARNING_RATE', str(self.learning_rate)))
        self.optimizer = os.getenv('MEMORAITH_OPTIMIZER', self.optimizer)
        self.loss_function = os.getenv('MEMORAITH_LOSS_FUNCTION', self.loss_function)

    def load_from_file(self, config_file: str) -> None:
        """Load configuration from a YAML file."""
        with open(config_file, 'r') as f:
            config_data = yaml.safe_load(f)

        for key, value in config_data.items():
            if hasattr(self, key):
                setattr(self, key, value)

    def save_to_file(self, filename: str) -> None:
        """Save the current configuration to a JSON file."""
        with open(filename, 'w') as f:
            json.dump(self.to_dict(), f, indent=2)
        logging.info(f"Configuration saved to {filename}")

    def get_optimizer(self, parameters: Any) -> Optional[Any]:
        """Get the optimizer instance based on the configuration."""
        optimizer_map = {
            'adam': optim.Adam,
            'sgd': optim.SGD,
            'rmsprop': optim.RMSprop
        }
        optimizer_class = optimizer_map.get(self.optimizer.lower())
        return optimizer_class(parameters, lr=self.learning_rate) if optimizer_class else None

    def get_loss_function(self) -> Optional[Any]:
        """Get the loss function based on the configuration."""
        loss_map = {
            'cross_entropy': nn.CrossEntropyLoss,
            'mse': nn.MSELoss,
            'bce': nn.BCELoss
        }
        loss_class = loss_map.get(self.loss_function.lower())
        return loss_class() if loss_class else None

    def to_dict(self) -> Dict[str, Any]:
        """Convert configuration to a dictionary."""
        return {k: str(v) if isinstance(v, Path) else v
                for k, v in self.__dict__.items()
                if not k.startswith('_')}

    def validate(self) -> bool:
        """Validate the current configuration."""
        valid = True
        if not isinstance(self.output_path, Path):
            logging.error("output_path must be a Path object")
            valid = False
        if not isinstance(self.enable_gpu, bool):
            logging.error("enable_gpu must be a boolean")
            valid = False
        return valid

# Global configuration instance
config = Config()
Class: Config
--------------------------------------------------------------------------------
  Method: __init__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\exceptions.py

class MemoraithError(Exception):
    """Base exception class for Memoraith."""
    def __init__(self, message: str):
        self.message = message
        super().__init__(self.message)

class FrameworkNotSupportedError(MemoraithError):
    """Exception raised when an unsupported framework is used."""
    def __init__(self, framework_name: str):
        self.framework_name = framework_name
        message = f"Framework '{framework_name}' is not supported."
        super().__init__(message)

class ConfigurationError(MemoraithError):
    """Exception raised when there's an issue with the configuration."""
    def __init__(self, config_item: str, details: str):
        self.config_item = config_item
        self.details = details
        message = f"Configuration error for {config_item}: {details}"
        super().__init__(message)

class ProfilingError(MemoraithError):
    """Exception raised when there's an error during profiling."""
    def __init__(self, component: str, details: str):
        self.component = component
        self.details = details
        message = f"Profiling error in {component}: {details}"
        super().__init__(message)

class DataCollectionError(MemoraithError):
    """Exception raised when there's an error collecting profiling data."""
    def __init__(self, data_type: str, details: str):
        self.data_type = data_type
        self.details = details
        message = f"Error collecting {data_type} data: {details}"
        super().__init__(message)

class AnalysisError(MemoraithError):
    """Exception raised when there's an error during data analysis."""
    def __init__(self, analysis_type: str, details: str):
        self.analysis_type = analysis_type
        self.details = details
        message = f"Error during {analysis_type} analysis: {details}"
        super().__init__(message)

class ReportGenerationError(MemoraithError):
    """Exception raised when there's an error generating reports."""
    def __init__(self, report_type: str, details: str):
        self.report_type = report_type
        self.details = details
        message = f"Error generating {report_type} report: {details}"
        super().__init__(message)

class GPUNotAvailableError(MemoraithError):
    """Exception raised when GPU profiling is requested but not available."""
    def __init__(self, details: str):
        self.details = details
        message = f"GPU profiling not available: {details}"
        super().__init__(message)
Class: MemoraithError
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__

Class: FrameworkNotSupportedError
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__

Class: ConfigurationError
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__

Class: ProfilingError
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__

Class: DataCollectionError
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__

Class: AnalysisError
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__

Class: ReportGenerationError
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__

Class: GPUNotAvailableError
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\logging_config.py

import logging
import sys
from typing import Optional
from pathlib import Path

def setup_logging(log_level: int, log_file: Optional[str] = None, log_format: Optional[str] = None):
    """
    Configure logging for Memoraith with enhanced features.

    Args:
        log_level (int): The logging level to set
        log_file (str, optional): Path to the log file. If None, logs to console only.
        log_format (str, optional): Custom log format. If None, uses a default format.
    """
    logger = logging.getLogger('memoraith')
    logger.setLevel(log_level)

    # Use custom format if provided, otherwise use a default
    if log_format is None:
        log_format = '[%(asctime)s] %(levelname)s [%(name)s.%(funcName)s:%(lineno)d] %(message)s'
    formatter = logging.Formatter(log_format, datefmt='%Y-%m-%d %H:%M:%S')

    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    # File handler (if log_file is provided)
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    # Suppress logs from other libraries
    for lib in ['matplotlib', 'PIL', 'tensorflow', 'torch']:
        logging.getLogger(lib).setLevel(logging.WARNING)

    logger.info("Logging initialized")

def get_logger(name: str) -> logging.Logger:
    """
    Get a logger with the given name.

    Args:
        name (str): Name of the logger

    Returns:
        logging.Logger: Logger instance
    """
    return logging.getLogger(f"memoraith.{name}")

def log_exception(logger: logging.Logger, exc: Exception, level: int = logging.ERROR):
    """
    Log an exception with full traceback.

    Args:
        logger (logging.Logger): Logger instance
        exc (Exception): Exception to log
        level (int): Logging level for the exception
    """
    logger.log(level, "Exception occurred", exc_info=True)

def create_log_directory(base_path: str) -> str:
    """
    Create a directory for log files if it doesn't exist.

    Args:
        base_path (str): Base path for creating the log directory

    Returns:
        str: Path to the created log directory
    """
    log_dir = Path(base_path) / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    return str(log_dir)

def set_log_level(logger: logging.Logger, level: int):
    """
    Set the logging level for a specific logger.

    Args:
        logger (logging.Logger): Logger instance
        level (int): Logging level to set
    """
    logger.setLevel(level)

def add_file_handler(logger: logging.Logger, file_path: str, level: int = logging.DEBUG):
    """
    Add a file handler to a logger.

    Args:
        logger (logging.Logger): Logger instance
        file_path (str): Path to the log file
        level (int): Logging level for the file handler
    """
    handler = logging.FileHandler(file_path)
    handler.setLevel(level)
    formatter = logging.Formatter('[%(asctime)s] %(levelname)s [%(name)s.%(funcName)s:%(lineno)d] %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\profiler.py

import functools
import logging
import asyncio
from typing import Callable, Any, Dict
from pathlib import Path

from memoraith.config import config
from memoraith.logging_config import setup_logging
from memoraith.integration import get_framework_adapter
from memoraith.analysis import Analyzer
from memoraith.reporting import ReportGenerator
from memoraith.exceptions import MemoraithError
from memoraith.visualization.real_time_visualizer import RealTimeVisualizer

def profile_model(
        memory: bool = True,
        computation: bool = True,
        gpu: bool = False,
        save_report: bool = True,
        report_format: str = 'html',
        real_time_viz: bool = False
) -> Callable:
    """
    Decorator to profile a model's training or inference function.

    Args:
        memory (bool): Enable memory profiling
        computation (bool): Enable computation time profiling
        gpu (bool): Enable GPU profiling
        save_report (bool): Save the profiling report
        report_format (str): Format of the saved report ('html' or 'pdf')
        real_time_viz (bool): Enable real-time visualization

    Returns:
        Callable: Decorated function
    """

    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(*args: Any, **kwargs: Any) -> Any:
            # Setup logging
            setup_logging(config.log_level)
            logger = logging.getLogger('memoraith')
            logger.info("Starting Memoraith Profiler...")

            # Update configuration
            config.enable_memory = memory
            config.enable_time = computation
            config.enable_gpu = gpu
            config.report_format = report_format

            visualizer = None
            try:
                # Get model from args or kwargs
                model = kwargs.get('model')
                if model is None and args:
                    model = args[0]

                if not model:
                    raise MemoraithError("No model provided for profiling")

                # Initialize framework adapter
                adapter = get_framework_adapter(model)

                # Initialize visualizer if requested
                if real_time_viz:
                    visualizer = RealTimeVisualizer()

                # Profile the model
                async with adapter:
                    # Execute the wrapped function
                    if asyncio.iscoroutinefunction(func):
                        result = await func(*args, **kwargs)
                    else:
                        result = await asyncio.to_thread(func, *args, **kwargs)

                    # Update visualization if enabled
                    if visualizer and adapter.data:
                        await visualizer.update(adapter.data)

                    # Analyze results
                    analyzer = Analyzer(adapter.data)
                    analysis_results = await analyzer.run_analysis()

                    # Generate report if requested
                    if save_report:
                        report_generator = ReportGenerator(analysis_results)
                        await report_generator.generate(format=report_format)

                    logger.info("Memoraith Profiling Completed Successfully")
                    return result

            except MemoraithError as e:
                logger.error(f"MemoraithError: {str(e)}")
                raise
            except Exception as e:
                logger.exception("An unexpected error occurred during profiling")
                raise
            finally:
                if visualizer:
                    try:
                        await visualizer.stop()
                    except Exception as e:
                        logger.error(f"Error stopping visualizer: {str(e)}")

        return wrapper
    return decorator

def set_output_path(path: str) -> None:
    """
    Set the output path for profiling reports.

    Args:
        path (str): Directory path for saving profiling outputs
    """
    try:
        output_path = Path(path)
        output_path.mkdir(parents=True, exist_ok=True)
        config.output_path = output_path
        logging.getLogger('memoraith').info(f"Output path set to: {output_path}")
    except Exception as e:
        raise MemoraithError(f"Failed to set output path: {str(e)}")

--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\__init__.py

"""
Memoraith: Lightweight Model Profiler
"""

__version__ = '0.5.0'

from memoraith.profiler import profile_model, set_output_path
from memoraith.config import Config
from memoraith.exceptions import MemoraithError
from memoraith.analysis import analyzer
# Expose main components
__all__ = [
    'profile_model',
    'set_output_path',
    'Config',
    'MemoraithError',
]

--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\analysis\analyzer.py

# memoraith/analysis/analyzer.py
from typing import Dict, Any, List
import logging
import numpy as np
from .metrics import MetricsCalculator
from .bottleneck import BottleneckDetector
from .recommendations import RecommendationEngine
from .anomaly_detection import AnomalyDetector

class Analyzer:
    """Main analysis coordinator for profiling data."""

    def __init__(self, profiling_data: Dict[str, Any]):
        self.data = profiling_data
        self.logger = logging.getLogger(__name__)
        self.metrics_calculator = MetricsCalculator(profiling_data)
        self.bottleneck_detector = BottleneckDetector()
        self.anomaly_detector = AnomalyDetector()
        self.recommendation_engine = RecommendationEngine()

    async def run_analysis(self) -> Dict[str, Any]:
        """Run complete analysis pipeline."""
        try:
            metrics = await self.metrics_calculator.calculate()
            if not metrics:
                raise ValueError("No metrics calculated")

            bottlenecks = await self.bottleneck_detector.detect(metrics)
            anomalies = await self.anomaly_detector.detect(metrics)
            recommendations = await self.recommendation_engine.generate(metrics)
            performance_score = self._calculate_performance_score(metrics)

            return {
                'metrics': metrics,
                'bottlenecks': bottlenecks,
                'anomalies': anomalies,
                'recommendations': recommendations,
                'performance_score': performance_score,
                'summary': await self._generate_summary(metrics)
            }
        except Exception as e:
            self.logger.error(f"Analysis failed: {str(e)}")
            raise

    def _calculate_performance_score(self, metrics: Dict[str, Any]) -> float:
        """Calculate overall performance score."""
        try:
            # Weight factors for different metrics
            weights = {
                'memory': 0.4,
                'compute': 0.4,
                'efficiency': 0.2
            }

            # Memory score (lower is better)
            memory_usage = metrics.get('peak_memory_percent', 0)
            memory_score = max(0, 100 - memory_usage)

            # Compute score (based on time efficiency)
            compute_time = metrics.get('total_time', 0)
            compute_score = 100 * np.exp(-compute_time / 10)  # Exponential decay

            # Efficiency score (based on resource utilization)
            efficiency_score = metrics.get('resource_efficiency', 80)

            # Calculate weighted score
            final_score = (
                    weights['memory'] * memory_score +
                    weights['compute'] * compute_score +
                    weights['efficiency'] * efficiency_score
            )

            return min(100, max(0, final_score))
        except Exception as e:
            self.logger.error(f"Error calculating performance score: {str(e)}")
            return 0.0

    async def _generate_summary(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Generate a concise summary of analysis results."""
        return {
            'total_time': metrics.get('total_time', 0),
            'peak_memory': metrics.get('peak_memory', 0),
            'gpu_utilization': metrics.get('gpu_utilization', 0),
            'bottleneck_count': len(metrics.get('bottlenecks', [])),
            'anomaly_count': len(metrics.get('anomalies', [])),
            'recommendation_count': len(metrics.get('recommendations', []))
        }

    async def export_results(self, format: str = 'json') -> Dict[str, Any]:
        """Export analysis results in specified format."""
        try:
            results = await self.run_analysis()
            if format == 'json':
                return results
            elif format == 'summary':
                return await self._generate_summary(results['metrics'])
            else:
                raise ValueError(f"Unsupported export format: {format}")
        except Exception as e:
            self.logger.error(f"Error exporting results: {str(e)}")
            raise
Class: Analyzer
--------------------------------------------------------------------------------
  Method: __init__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\analysis\anomaly_detection.py

import numpy as np
from typing import Dict, Any, List
import logging

class AnomalyDetector:
    """Detects anomalies in the performance metrics."""

    def __init__(self, z_threshold: float = 3.0):
        self.z_threshold = z_threshold
        self.logger = logging.getLogger(__name__)

    async def detect(self, metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Detect anomalies such as sudden spikes in memory or time.

        Args:
            metrics (Dict[str, Any]): Calculated metrics for each layer

        Returns:
            List[Dict[str, Any]]: List of detected anomalies
        """
        anomalies = []
        layers = list(metrics.keys())

        try:
            cpu_memory = [metrics[layer].get('total_cpu_memory', 0) for layer in layers]
            gpu_memory = [metrics[layer].get('total_gpu_memory', 0) for layer in layers]
            time = [metrics[layer].get('total_time', 0) for layer in layers]

            anomalies.extend(self._detect_anomalies(cpu_memory, 'CPU Memory', layers))
            anomalies.extend(self._detect_anomalies(gpu_memory, 'GPU Memory', layers))
            anomalies.extend(self._detect_anomalies(time, 'Computation Time', layers))
        except Exception as e:
            self.logger.error(f"Error during anomaly detection: {str(e)}")

        return anomalies

    def _detect_anomalies(self, data: List[float], data_type: str, layers: List[str]) -> List[Dict[str, Any]]:
        """Helper method to detect anomalies using z-score."""
        anomalies = []
        if not data:
            return anomalies

        mean = np.mean(data)
        std = np.std(data)

        if std == 0:
            return anomalies  # No variation, so no anomalies

        z_scores = [(x - mean) / std for x in data]

        for layer, z_score, value in zip(layers, z_scores, data):
            if abs(z_score) > self.z_threshold:
                anomalies.append({
                    'layer': layer,
                    'type': data_type,
                    'value': value,
                    'z_score': z_score
                })

        return anomalies
Class: AnomalyDetector
--------------------------------------------------------------------------------
  Method: __init__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\analysis\bottleneck.py

import logging
import numpy as np
import matplotlib.pyplot as plt
import aiofiles
from typing import Dict, Any, List

class BottleneckDetector:
    def __init__(self, time_threshold: float = 0.1, memory_threshold: float = 0.2):
        self.time_threshold = time_threshold
        self.memory_threshold = memory_threshold
        self.logger = logging.getLogger(__name__)

    async def detect(self, metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detect performance bottlenecks"""
        bottlenecks = []
        try:
            total_time = sum(metrics.get('time_metrics', {}).values())
            total_memory = sum(metrics.get('memory_metrics', {}).values())

            for component, values in metrics.items():
                if isinstance(values, dict):
                    time_ratio = values.get('duration', 0) / total_time if total_time else 0
                    memory_ratio = values.get('memory_usage', 0) / total_memory if total_memory else 0

                    if time_ratio > self.time_threshold:
                        bottlenecks.append({
                            'component': component,
                            'type': 'time',
                            'value': time_ratio,
                            'severity': 'high' if time_ratio > 0.5 else 'medium'
                        })

                    if memory_ratio > self.memory_threshold:
                        bottlenecks.append({
                            'component': component,
                            'type': 'memory',
                            'value': memory_ratio,
                            'severity': 'high' if memory_ratio > 0.5 else 'medium'
                        })

            return bottlenecks
        except Exception as e:
            self.logger.error(f"Bottleneck detection failed: {str(e)}")
            return []

"""
memoraith/analysis/anomaly_detection.py
"""
class AnomalyDetector:
    def __init__(self, threshold: float = 2.0):
        self.threshold = threshold
        self.logger = logging.getLogger(__name__)

    async def detect(self, metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detect anomalies in metrics using z-score"""
        anomalies = []
        try:
            for metric_type, values in metrics.items():
                if isinstance(values, dict):
                    mean = np.mean(list(values.values()))
                    std = np.std(list(values.values()))

                    if std == 0:
                        continue

                    for component, value in values.items():
                        z_score = (value - mean) / std
                        if abs(z_score) > self.threshold:
                            anomalies.append({
                                'component': component,
                                'metric_type': metric_type,
                                'value': value,
                                'z_score': z_score
                            })

            return anomalies
        except Exception as e:
            self.logger.error(f"Anomaly detection failed: {str(e)}")
            return []

"""
memoraith/analysis/recommendations.py
"""
class RecommendationEngine:
    def __init__(self):
        self.logger = logging.getLogger(__name__)

    async def generate(self, metrics: Dict[str, Any]) -> List[Dict[str, str]]:
        """Generate optimization recommendations"""
        recommendations = []
        try:
            # Check memory usage
            if metrics.get('peak_memory_percent', 0) > 80:
                recommendations.append({
                    'type': 'memory',
                    'recommendation': 'Consider implementing memory optimization techniques'
                })

            # Check network usage
            if metrics.get('peak_bandwidth', 0) > 100:
                recommendations.append({
                    'type': 'network',
                    'recommendation': 'Consider implementing network traffic optimization'
                })

            # Check response times
            if metrics.get('average_response_time', 0) > 1.0:
                recommendations.append({
                    'type': 'performance',
                    'recommendation': 'Consider implementing caching or optimization techniques'
                })

            return recommendations
        except Exception as e:
            self.logger.error(f"Recommendation generation failed: {str(e)}")
            return []

"""
memoraith/visualization/real_time_visualizer.py
"""
class RealTimeVisualizer:
    def __init__(self):
        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1, figsize=(10, 8))
        self.memory_data = []
        self.network_data = []
        plt.ion()

    async def update(self, metrics: Dict[str, Any]) -> None:
        """Update visualization with new metrics"""
        try:
            self.memory_data.append(metrics.get('memory', {}).get('usage', 0))
            self.network_data.append(metrics.get('network', {}).get('bandwidth', 0))

            # Update memory plot
            self.ax1.clear()
            self.ax1.plot(self.memory_data)
            self.ax1.set_title('Memory Usage')
            self.ax1.set_ylabel('MB')

            # Update network plot
            self.ax2.clear()
            self.ax2.plot(self.network_data)
            self.ax2.set_title('Network Bandwidth')
            self.ax2.set_ylabel('Mbps')

            plt.tight_layout()
            plt.draw()
            plt.pause(0.1)

        except Exception as e:
            self.logger.error(f"Visualization update failed: {str(e)}")

    def close(self):
        """Close visualization window"""
        plt.close(self.fig)

"""
memoraith/reporting/console_report.py
"""
class ConsoleReport:
    def __init__(self, data: Dict[str, Any]):
        self.data = data
        self.logger = logging.getLogger(__name__)

    def display(self) -> None:
        """Display formatted report in console"""
        try:
            print("\n=== Performance Report ===")

            print("\nMemory Usage:")
            print(f"Peak: {self.data.get('peak_memory', 0):.2f} MB")
            print(f"Average: {self.data.get('average_memory', 0):.2f} MB")

            print("\nNetwork Usage:")
            print(f"Peak Bandwidth: {self.data.get('peak_bandwidth', 0):.2f} Mbps")
            print(f"Total Transfer: {self.data.get('total_bytes_transferred', 0) / 1024 / 1024:.2f} MB")

            print("\nBottlenecks:", len(self.data.get('bottlenecks', [])))
            print("Anomalies:", len(self.data.get('anomalies', [])))
            print("Recommendations:", len(self.data.get('recommendations', [])))

        except Exception as e:
            self.logger.error(f"Report display failed: {str(e)}")

"""
memoraith/reporting/report_generator.py
"""
class ReportGenerator:
    def __init__(self, data: Dict[str, Any]):
        self.data = data
        self.logger = logging.getLogger(__name__)

    async def generate(self, output_path: str) -> None:
        """Generate HTML report"""
        try:
            report_content = self._generate_html()

            async with aiofiles.open(output_path, 'w') as f:
                await f.write(report_content)

            self.logger.info(f"Report generated: {output_path}")

        except Exception as e:
            self.logger.error(f"Report generation failed: {str(e)}")
            raise

    def _generate_html(self) -> str:
        """Generate HTML content"""
        return f"""
        <html>
            <head>
                <title>Performance Report</title>
            </head>
            <body>
                <h1>Performance Report</h1>
                <h2>Memory Usage</h2>
                <p>Peak: {self.data.get('peak_memory', 0):.2f} MB</p>
                <p>Average: {self.data.get('average_memory', 0):.2f} MB</p>
                
                <h2>Network Usage</h2>
                <p>Peak Bandwidth: {self.data.get('peak_bandwidth', 0):.2f} Mbps</p>
                <p>Total Transfer: {self.data.get('total_bytes_transferred', 0) / 1024 / 1024:.2f} MB</p>
                
                <h2>Issues</h2>
                <p>Bottlenecks: {len(self.data.get('bottlenecks', []))}</p>
                <p>Anomalies: {len(self.data.get('anomalies', []))}</p>
                <p>Recommendations: {len(self.data.get('recommendations', []))}</p>
            </body>
        </html>
        """
Class: BottleneckDetector
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: close
  Method: __init__
  Method: __init__

Class: AnomalyDetector
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: close
  Method: __init__
  Method: __init__

Class: RecommendationEngine
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: close
  Method: __init__
  Method: __init__

Class: RealTimeVisualizer
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: close
  Method: __init__
  Method: __init__

Class: ConsoleReport
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: close
  Method: __init__
  Method: __init__

Class: ReportGenerator
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: close
  Method: __init__
  Method: __init__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\analysis\metrics.py


import logging
from typing import Dict, Any

class MetricsCalculator:
    def __init__(self, data: Dict[str, Any]):
        self.data = data
        self.logger = logging.getLogger(__name__)

    async def calculate(self) -> Dict[str, Any]:
        """Calculate key performance metrics"""
        try:
            # Process raw data
            network_metrics = self._calculate_network_metrics()
            memory_metrics = self._calculate_memory_metrics()
            time_metrics = self._calculate_time_metrics()

            return {
                **network_metrics,
                **memory_metrics,
                **time_metrics,
                'global_metrics': self._calculate_global_metrics()
            }
        except Exception as e:
            self.logger.error(f"Metrics calculation failed: {str(e)}")
            return {}

    def _calculate_network_metrics(self) -> Dict[str, float]:
        """Calculate network-related metrics"""
        network_data = self.data.get('network', {})
        return {
            'peak_bandwidth': max((m.get('bandwidth_mbps', 0) for m in network_data), default=0),
            'average_bandwidth': sum((m.get('bandwidth_mbps', 0) for m in network_data)) / len(network_data) if network_data else 0,
            'total_bytes_transferred': sum((m.get('bytes_sent', 0) + m.get('bytes_recv', 0) for m in network_data))
        }

    def _calculate_memory_metrics(self) -> Dict[str, float]:
        """Calculate memory-related metrics"""
        memory_data = self.data.get('memory', {})
        return {
            'peak_memory': max((m.get('rss', 0) for m in memory_data), default=0),
            'average_memory': sum((m.get('rss', 0) for m in memory_data)) / len(memory_data) if memory_data else 0,
            'peak_memory_percent': max((m.get('memory_percent', 0) for m in memory_data), default=0)
        }

    def _calculate_time_metrics(self) -> Dict[str, float]:
        """Calculate time-related metrics"""
        time_data = self.data.get('time', {})
        return {
            'total_time': sum((m.get('duration', 0) for m in time_data)),
            'average_response_time': sum((m.get('duration', 0) for m in time_data)) / len(time_data) if time_data else 0
        }

    def _calculate_global_metrics(self) -> Dict[str, Any]:
        """Calculate global performance metrics"""
        return {
            'start_time': min((m.get('timestamp', 0) for m in self.data.get('time', [])), default=0),
            'end_time': max((m.get('timestamp', 0) for m in self.data.get('time', [])), default=0),
            'total_samples': len(self.data.get('time', [])),
            'success_rate': self._calculate_success_rate()
        }

    def _calculate_success_rate(self) -> float:
        """Calculate the overall success rate of operations"""
        total = len(self.data.get('time', []))
        if not total:
            return 0.0
        failures = sum(1 for m in self.data.get('time', []) if m.get('error', False))
        return (total - failures) / total * 100
Class: MetricsCalculator
--------------------------------------------------------------------------------
  Method: __init__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\analysis\recommendations.py

from typing import Dict, Any, List
import logging

class RecommendationEngine:
    """Provides optimization suggestions based on detected bottlenecks."""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    async def generate(self, metrics: Dict[str, Any]) -> List[Dict[str, str]]:
        """
        Generate recommendations for each bottleneck.

        Args:
            metrics (Dict[str, Any]): Calculated metrics for each layer

        Returns:
            List[Dict[str, str]]: List of recommendations
        """
        recommendations = []
        try:
            for layer, layer_metrics in metrics.items():
                if not isinstance(layer_metrics, dict):
                    continue

                layer_recommendations = await self.generate_for_layer(layer, layer_metrics)
                recommendations.extend(layer_recommendations)
        except Exception as e:
            self.logger.error(f"Error generating recommendations: {str(e)}")

        return recommendations

    async def generate_for_layer(self, layer: str, layer_metrics: Dict[str, Any]) -> List[Dict[str, str]]:
        """
        Generate recommendations for a specific layer.

        Args:
            layer (str): Name of the layer
            layer_metrics (Dict[str, Any]): Metrics for the specific layer

        Returns:
            List[Dict[str, str]]: List of recommendations for the layer
        """
        recommendations = []

        try:
            if layer_metrics['total_time'] > 0.1:  # Arbitrary threshold
                recommendations.append({
                    'layer': layer,
                    'recommendation': f"Consider optimizing the {layer} for speed. It's taking {layer_metrics['total_time']:.4f} seconds."
                })

            if layer_metrics['total_cpu_memory'] > 1000:  # Arbitrary threshold (1000 MB)
                recommendations.append({
                    'layer': layer,
                    'recommendation': f"The {layer} is using a lot of memory ({layer_metrics['total_cpu_memory']:.2f} MB). Consider reducing its size or using more memory-efficient operations."
                })

            if 'parameters' in layer_metrics and layer_metrics['parameters'] > 1000000:  # Arbitrary threshold (1M parameters)
                recommendations.append({
                    'layer': layer,
                    'recommendation': f"The {layer} has a large number of parameters ({layer_metrics['parameters']:,}). Consider using techniques like pruning or quantization to reduce model size."
                })
        except KeyError as e:
            self.logger.error(f"Missing key in layer metrics for {layer}: {str(e)}")
        except Exception as e:
            self.logger.error(f"Error generating recommendations for {layer}: {str(e)}")

        return recommendations
Class: RecommendationEngine
--------------------------------------------------------------------------------
  Method: __init__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\analysis\__init__.py

from memoraith.analysis import analyzer
from .metrics import MetricsCalculator
from .bottleneck import BottleneckDetector
from .recommendations import RecommendationEngine
from .anomaly_detection import AnomalyDetector


class Analyzer:
    import analyzer
    pass
Class: Analyzer
--------------------------------------------------------------------------------


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\data_collection\base_collector.py

# memoraith/data_collection/base_collector.py
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import asyncio
import logging
from ..exceptions import DataCollectionError

class BaseDataCollector(ABC):
    """Base class for all data collectors."""

    def __init__(self, interval: float = 0.1):
        self.interval = interval
        self.logger = logging.getLogger(__name__)
        self.data: Dict[str, Any] = {}
        self._is_collecting = False
        self._task: Optional[asyncio.Task] = None
        self._lock = asyncio.Lock()

    async def start(self) -> None:
        """Start data collection."""
        try:
            async with self._lock:
                if self._is_collecting:
                    raise DataCollectionError("Collection already running")
                self._is_collecting = True
                self._task = asyncio.create_task(self._collection_loop())
                self.logger.info(f"{self.__class__.__name__} started collecting data")
        except Exception as e:
            self.logger.error(f"Failed to start collection: {str(e)}")
            raise DataCollectionError(f"Start failed: {str(e)}")

    async def stop(self) -> None:
        """Stop data collection."""
        try:
            async with self._lock:
                if not self._is_collecting:
                    return
                self._is_collecting = False
                if self._task:
                    self._task.cancel()
                    try:
                        await self._task
                    except asyncio.CancelledError:
                        pass
                    self._task = None
                self.logger.info(f"{self.__class__.__name__} stopped collecting data")
        except Exception as e:
            self.logger.error(f"Failed to stop collection: {str(e)}")
            raise DataCollectionError(f"Stop failed: {str(e)}")

    @abstractmethod
    async def _collect_data(self) -> Dict[str, Any]:
        """Collect a single data point."""
        raise NotImplementedError

    async def _collection_loop(self) -> None:
        """Main collection loop."""
        while self._is_collecting:
            try:
                data_point = await self._collect_data()
                async with self._lock:
                    self._process_data_point(data_point)
                await asyncio.sleep(self.interval)
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Error in collection loop: {str(e)}")

    def _process_data_point(self, data_point: Dict[str, Any]) -> None:
        """Process and store a collected data point."""
        timestamp = data_point.get('timestamp', 0)
        if timestamp not in self.data:
            self.data[timestamp] = data_point

    async def get_data(self) -> Dict[str, Any]:
        """Get collected data."""
        async with self._lock:
            return self.data.copy()

    async def clear_data(self) -> None:
        """Clear collected data."""
        async with self._lock:
            self.data.clear()

    @abstractmethod
    async def validate_data(self) -> bool:
        """Validate collected data."""
        raise NotImplementedError
Class: BaseDataCollector
--------------------------------------------------------------------------------
  Method: __init__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\data_collection\cpu_memory.py

import psutil
import threading
import time
from typing import List, Dict, Any
import logging

class CPUMemoryTracker:
    """
    Advanced CPU memory usage tracker with detailed memory breakdown.
    """

    def __init__(self, interval: float = 0.1, detailed: bool = True):
        self.interval = interval
        self.detailed = detailed
        self.memory_usage: List[Dict[str, Any]] = []
        self._stop_event = threading.Event()
        self._lock = threading.Lock()
        self._thread: threading.Thread = None
        self.logger = logging.getLogger(__name__)

    def start(self) -> None:
        """Start tracking CPU memory usage."""
        self._thread = threading.Thread(target=self._track_memory, daemon=True)
        self._thread.start()
        self.logger.info("CPU memory tracking started")

    def stop(self) -> None:
        """Stop tracking CPU memory usage."""
        self._stop_event.set()
        if self._thread:
            self._thread.join()
        self.logger.info("CPU memory tracking stopped")

    def _track_memory(self) -> None:
        process = psutil.Process()
        while not self._stop_event.is_set():
            try:
                mem_info = process.memory_info()
                mem_data = {
                    'timestamp': time.time(),
                    'rss': mem_info.rss / (1024 * 1024),  # RSS in MB
                    'vms': mem_info.vms / (1024 * 1024),  # VMS in MB
                }

                if self.detailed:
                    mem_maps = process.memory_maps(grouped=True)
                    mem_data.update({
                        'shared': sum(m.shared for m in mem_maps) / (1024 * 1024),
                        'private': sum(m.private for m in mem_maps) / (1024 * 1024),
                        'swap': sum(m.swap for m in mem_maps) / (1024 * 1024),
                    })

                with self._lock:
                    self.memory_usage.append(mem_data)
            except Exception as e:
                self.logger.error(f"Error tracking CPU memory: {str(e)}")

            time.sleep(self.interval)

    def get_peak_memory(self) -> Dict[str, float]:
        """Get the peak memory usage."""
        with self._lock:
            if not self.memory_usage:
                return {}
            return max(self.memory_usage, key=lambda x: x['rss'])

    def get_average_memory(self) -> Dict[str, float]:
        """Get the average memory usage."""
        with self._lock:
            if not self.memory_usage:
                return {}
            avg_mem = {key: sum(m[key] for m in self.memory_usage) / len(self.memory_usage)
                       for key in self.memory_usage[0] if key != 'timestamp'}
            return avg_mem

    def get_memory_timeline(self) -> List[Dict[str, Any]]:
        """Get the full timeline of memory usage."""
        with self._lock:
            return self.memory_usage.copy()

    def reset(self) -> None:
        """Reset the memory usage data."""
        with self._lock:
            self.memory_usage.clear()
        self.logger.info("CPU memory tracking data reset")
Class: CPUMemoryTracker
--------------------------------------------------------------------------------
  Method: __init__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\data_collection\gpu_memory.py

import logging
from typing import List, Optional
import asyncio

logger = logging.getLogger(__name__)

try:
    from pynvml import *
    PYNVML_AVAILABLE = True
except ImportError:
    logger.warning("pynvml not available. GPU memory tracking will be disabled.")
    PYNVML_AVAILABLE = False

from ..exceptions import GPUNotAvailableError

class GPUMemoryTracker:
    """Tracks GPU memory usage over time for NVIDIA GPUs."""

    def __init__(self, device_id: int = 0, interval: float = 0.1):
        self.device_id = device_id
        self.interval = interval
        self.memory_usage: List[float] = []
        self._stop_event: Optional[asyncio.Event] = None
        self._tracking_task: Optional[asyncio.Task] = None
        self.logger = logging.getLogger(__name__)

    async def start(self) -> None:
        """Start tracking GPU memory usage."""
        if not PYNVML_AVAILABLE:
            self.logger.warning("GPU memory tracking is not available.")
            return

        try:
            nvmlInit()
            self.device = nvmlDeviceGetHandleByIndex(self.device_id)
            self._stop_event = asyncio.Event()
            self._tracking_task = asyncio.create_task(self._track_memory())
            self.logger.info(f"Started GPU memory tracking for device {self.device_id}")
        except NVMLError as e:
            self.logger.error(f"Failed to initialize GPU memory tracking: {str(e)}")
            raise GPUNotAvailableError(f"GPU tracking failed: {str(e)}")

    async def stop(self) -> None:
        """Stop tracking GPU memory usage."""
        if not PYNVML_AVAILABLE:
            return

        if self._stop_event:
            self._stop_event.set()
        if self._tracking_task:
            await self._tracking_task
        try:
            nvmlShutdown()
            self.logger.info("Stopped GPU memory tracking")
        except NVMLError as e:
            self.logger.error(f"Error during NVML shutdown: {str(e)}")

    async def _track_memory(self) -> None:
        """Internal method to track memory usage."""
        while not self._stop_event.is_set():
            try:
                mem_info = nvmlDeviceGetMemoryInfo(self.device)
                self.memory_usage.append(mem_info.used / 1024**2)  # Convert to MB
            except NVMLError as e:
                self.logger.error(f"Error tracking GPU memory: {str(e)}")
            await asyncio.sleep(self.interval)

    async def get_peak_memory(self) -> float:
        """Get the peak GPU memory usage."""
        return max(self.memory_usage) if self.memory_usage else 0

    async def get_average_memory(self) -> float:
        """Get the average GPU memory usage."""
        return sum(self.memory_usage) / len(self.memory_usage) if self.memory_usage else 0

    async def get_current_memory(self) -> float:
        """Get the current GPU memory usage."""
        if not PYNVML_AVAILABLE:
            return 0

        try:
            mem_info = nvmlDeviceGetMemoryInfo(self.device)
            return mem_info.used / 1024**2  # Convert to MB
        except NVMLError as e:
            self.logger.error(f"Error getting current GPU memory: {str(e)}")
            return 0

    async def get_memory_history(self) -> List[float]:
        """Get the full history of memory usage."""
        return self.memory_usage

    async def get_device_info(self) -> dict:
        """Get information about the GPU device being tracked."""
        if not PYNVML_AVAILABLE:
            return {"error": "GPU information not available"}

        try:
            device_name = nvmlDeviceGetName(self.device).decode('utf-8')
            total_memory = nvmlDeviceGetMemoryInfo(self.device).total / 1024**2  # Convert to MB
            cuda_version = nvmlSystemGetCudaDriverVersion()
            return {
                "device_id": self.device_id,
                "device_name": device_name,
                "total_memory": total_memory,
                "cuda_version": f"{cuda_version // 1000}.{(cuda_version % 1000) // 10}"
            }
        except NVMLError as e:
            self.logger.error(f"Error getting GPU device info: {str(e)}")
            return {"error": str(e)}

    async def reset(self) -> None:
        """Reset the memory usage history."""
        self.memory_usage = []

    def __del__(self):
        """Ensure NVML is shut down when the object is deleted."""
        if PYNVML_AVAILABLE:
            try:
                nvmlShutdown()
            except:
                pass  # Ignore errors during shutdown in destructor
Class: GPUMemoryTracker
--------------------------------------------------------------------------------
  Method: __init__
  Method: __del__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\data_collection\network_memory_system.py

"""
Complete Network and Memory Profiling System
Filename: memoraith/data_collection/network_memory_system.py
"""

import os
import psutil
import threading
import tarfile
import asyncio
import logging
import time
from typing import Dict, Any, Optional, List, Union, Callable
from dataclasses import dataclass
from enum import Enum, auto
from contextlib import asynccontextmanager, contextmanager
import json
from pathlib import Path
import aiofiles
import numpy as np
from datetime import datetime

# Core Exceptions
class ProfilingError(Exception):
    """to do base exception for profiling errors"""
    pass

class ResourceError(ProfilingError):
    """to do Resource management related errors"""
    pass

class NetworkError(ProfilingError):
    """to do Network-specific profiling errors"""
    pass

class MemoryError(ProfilingError):
    """to do Memory-specific profiling errors"""
    pass

# Configuration Classes
class ProfilingType(Enum):
    NETWORK = auto()
    MEMORY = auto()
    BOTH = auto()

@dataclass
class ProfilingConfig:
    """Configuration for the profiling system"""
    interval: float = 0.1
    detailed: bool = True
    log_directory: str = "profiling_logs"
    max_samples: int = 10000
    enable_disk_logging: bool = True
    enable_console_output: bool = True
    async_mode: bool = True
    memory_threshold_mb: float = 1000.0
    network_threshold_mbps: float = 100.0
    alert_enabled: bool = True
    compression_enabled: bool = True
    backup_enabled: bool = True
    retention_days: int = 7

class MetricCollector:
    """Base class for metric collection"""

    def __init__(self, config: ProfilingConfig):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.metrics: List[Dict[str, Any]] = []
        self._lock = threading.Lock()
        self._async_lock = asyncio.Lock()
        self.is_collecting = False
        self.start_time: Optional[float] = None

    async def start(self) -> None:
        """Start metric collection"""
        raise NotImplementedError

    async def stop(self) -> None:
        """Stop metric collection"""
        raise NotImplementedError

    async def get_metrics(self) -> Dict[str, Any]:
        """Get collected metrics"""
        raise NotImplementedError

    def _setup_logging(self, name: str) -> None:
        """Setup logging configuration"""
        log_path = Path(self.config.log_directory) / f"{name}.log"
        log_path.parent.mkdir(parents=True, exist_ok=True)

        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

        if self.config.enable_disk_logging:
            file_handler = logging.FileHandler(log_path)
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)

        if self.config.enable_console_output:
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)

        self.logger.setLevel(logging.DEBUG)

class NetworkMetrics(MetricCollector):
    """Network metrics collection implementation"""

    def __init__(self, config: ProfilingConfig):
        super().__init__(config)
        self._setup_logging("network")
        self.baseline_counters: Optional[psutil._common.snetio] = None
        self.task: Optional[asyncio.Task] = None
        self._setup_storage()

    def _setup_storage(self) -> None:
        """Initialize storage for network metrics"""
        self.storage_path = Path(self.config.log_directory) / "network_metrics"
        self.storage_path.mkdir(parents=True, exist_ok=True)

    async def start(self) -> None:
        """Start network metrics collection"""
        try:
            async with self._async_lock:
                if self.is_collecting:
                    raise NetworkError("Network metrics collection already running")

                self.start_time = time.time()
                self.is_collecting = True
                self.metrics.clear()
                self.baseline_counters = psutil.net_io_counters()

                if self.config.async_mode:
                    self.task = asyncio.create_task(self._collect_metrics_async())
                else:
                    threading.Thread(
                        target=self._collect_metrics_sync,
                        daemon=True
                    ).start()

                self.logger.info("Network metrics collection started")
                await self._save_state("started")

        except Exception as e:
            self.logger.error(f"Failed to start network metrics collection: {e}", exc_info=True)
            self.is_collecting = False
            raise NetworkError(f"Failed to start network collection: {str(e)}") from e

    async def stop(self) -> Dict[str, Any]:
        """Stop network metrics collection and return results"""
        try:
            async with self._async_lock:
                if not self.is_collecting:
                    raise NetworkError("Network metrics collection not running")

                self.is_collecting = False
                if self.task:
                    await self.task
                    self.task = None

                results = await self._generate_report()
                await self._save_state("stopped")
                return results

        except Exception as e:
            self.logger.error(f"Error stopping network metrics collection: {e}", exc_info=True)
            raise NetworkError(f"Failed to stop network collection: {str(e)}") from e

    async def _collect_metrics_async(self) -> None:
        """Asynchronous network metrics collection"""
        sample_count = 0
        last_counters = self.baseline_counters

        while self.is_collecting and sample_count < self.config.max_samples:
            try:
                await asyncio.sleep(self.config.interval)
                current_counters = psutil.net_io_counters()

                metrics = await self._calculate_metrics(last_counters, current_counters)
                await self._store_metrics(metrics)

                if await self._should_alert(metrics):
                    await self._send_alert(metrics)

                last_counters = current_counters
                sample_count += 1

            except Exception as e:
                self.logger.error(f"Error in async network collection: {e}", exc_info=True)
                await asyncio.sleep(1)

    def _collect_metrics_sync(self) -> None:
        """Synchronous network metrics collection"""
        sample_count = 0
        last_counters = self.baseline_counters

        while self.is_collecting and sample_count < self.config.max_samples:
            try:
                time.sleep(self.config.interval)
                current_counters = psutil.net_io_counters()

                metrics = self._calculate_metrics_sync(last_counters, current_counters)
                self._store_metrics_sync(metrics)

                if self._should_alert_sync(metrics):
                    self._send_alert_sync(metrics)

                last_counters = current_counters
                sample_count += 1

            except Exception as e:
                self.logger.error(f"Error in sync network collection: {e}", exc_info=True)
                time.sleep(1)

    async def _calculate_metrics(
            self,
            last_counters: psutil._common.snetio,
            current_counters: psutil._common.snetio
    ) -> Dict[str, Any]:
        """Calculate network metrics between two counter states"""
        metrics = {
            'timestamp': time.time(),
            'bytes_sent': current_counters.bytes_sent - last_counters.bytes_sent,
            'bytes_recv': current_counters.bytes_recv - last_counters.bytes_recv,
            'packets_sent': current_counters.packets_sent - last_counters.packets_sent,
            'packets_recv': current_counters.packets_recv - last_counters.packets_recv,
            'bandwidth_mbps': (
                    (current_counters.bytes_sent + current_counters.bytes_recv -
                     last_counters.bytes_sent - last_counters.bytes_recv) * 8 /
                    (1024 * 1024 * self.config.interval)
            )
        }

        if self.config.detailed:
            metrics.update({
                'errin': current_counters.errin - last_counters.errin,
                'errout': current_counters.errout - last_counters.errout,
                'dropin': current_counters.dropin - last_counters.dropin,
                'dropout': current_counters.dropout - last_counters.dropout,
                'error_rate': self._calculate_error_rate(
                    current_counters, last_counters
                )
            })

        return metrics

    def _calculate_metrics_sync(
            self,
            last_counters: psutil._common.snetio,
            current_counters: psutil._common.snetio
    ) -> Dict[str, Any]:
        """Synchronous version of metrics calculation"""
        return {
            'timestamp': time.time(),
            'bytes_sent': current_counters.bytes_sent - last_counters.bytes_sent,
            'bytes_recv': current_counters.bytes_recv - last_counters.bytes_recv,
            'packets_sent': current_counters.packets_sent - last_counters.packets_sent,
            'packets_recv': current_counters.packets_recv - last_counters.packets_recv,
            'bandwidth_mbps': (
                    (current_counters.bytes_sent + current_counters.bytes_recv -
                     last_counters.bytes_sent - last_counters.bytes_recv) * 8 /
                    (1024 * 1024 * self.config.interval)
            )
        }

    async def _store_metrics(self, metrics: Dict[str, Any]) -> None:
        """Store network metrics with async file writing"""
        async with self._async_lock:
            self.metrics.append(metrics)

            if self.config.enable_disk_logging:
                file_path = self.storage_path / f"network_metrics_{int(self.start_time)}.json"
                async with aiofiles.open(file_path, 'a') as f:
                    await f.write(json.dumps(metrics) + '\n')

                if self.config.compression_enabled:
                    await self._compress_old_metrics()

    def _store_metrics_sync(self, metrics: Dict[str, Any]) -> None:
        """Synchronous version of metrics storage"""
        with self._lock:
            self.metrics.append(metrics)

            if self.config.enable_disk_logging:
                file_path = self.storage_path / f"network_metrics_{int(self.start_time)}.json"
                with open(file_path, 'a') as f:
                    json.dump(metrics, f)
                    f.write('\n')

    async def _should_alert(self, metrics: Dict[str, Any]) -> bool:
        """Check if metrics should trigger an alert"""
        if not self.config.alert_enabled:
            return False

        return (
                metrics['bandwidth_mbps'] > self.config.network_threshold_mbps or
                metrics.get('error_rate', 0) > 0.1  # 10% error rate threshold
        )

    async def _send_alert(self, metrics: Dict[str, Any]) -> None:
        """Send alert for concerning metrics"""
        alert_message = (
            f"Network Alert: Bandwidth {metrics['bandwidth_mbps']:.2f} Mbps "
            f"exceeds threshold {self.config.network_threshold_mbps} Mbps"
        )
        self.logger.warning(alert_message)
        # Additional alert mechanisms could be added here (email, SMS, etc.)

    async def _compress_old_metrics(self) -> None:
        """Compress old metric files to save space"""
        import gzip
        import shutil

        for file_path in self.storage_path.glob("*.json"):
            if time.time() - file_path.stat().st_mtime > 86400:  # 24 hours
                gz_path = file_path.with_suffix('.json.gz')
                with open(file_path, 'rb') as f_in:
                    with gzip.open(gz_path, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                file_path.unlink()

    async def _generate_report(self) -> Dict[str, Any]:
        """Generate comprehensive network metrics report"""
        if not self.metrics:
            return self._generate_empty_report()

        total_metrics = self._calculate_total_metrics()
        peak_metrics = self._calculate_peak_metrics()
        average_metrics = self._calculate_average_metrics()

        report = {
            'start_time': self.start_time,
            'end_time': time.time(),
            'duration': time.time() - self.start_time,
            'total_samples': len(self.metrics),
            'total_metrics': total_metrics,
            'peak_metrics': peak_metrics,
            'average_metrics': average_metrics,
            'bandwidth_statistics': self._calculate_bandwidth_statistics(),
            'error_statistics': self._calculate_error_statistics()
        }

        # Save report to disk
        report_path = self.storage_path / f"network_report_{int(self.start_time)}.json"
        async with aiofiles.open(report_path, 'w') as f:
            await f.write(json.dumps(report, indent=2))

        return report

    def _generate_empty_report(self) -> Dict[str, Any]:
        """Generate empty report when no metrics collected"""
        return {
            'start_time': self.start_time,
            'end_time': time.time(),
            'duration': time.time() - self.start_time,
            'total_samples': 0,
            'error': 'No metrics collected during the session'
        }

    def _calculate_error_rate(
            self,
            current: psutil._common.snetio,
            last: psutil._common.snetio
    ) -> float:
        """Calculate error rate between two counter states"""
        total_packets = (
                (current.packets_sent + current.packets_recv) -
                (last.packets_sent + last.packets_recv)
        )

        if total_packets == 0:
            return 0.0

        total_errors = (
                (current.errin + current.errout + current.dropin + current.dropout) -
                (last.errin + last.errout + last.dropin + last.dropout)
        )

        return (total_errors / total_packets) * 100 if total_packets > 0 else 0

    def _calculate_total_metrics(self) -> Dict[str, Any]:
        """Calculate total metrics for the session"""
        return {
            key: sum(m[key] for m in self.metrics if key != 'timestamp')
            for key in self.metrics[0].keys()
            if key != 'timestamp'
        }

    def _calculate_peak_metrics(self) -> Dict[str, Any]:
        """Calculate peak metrics for the session"""
        return {
            key: max(m[key] for m in self.metrics if key != 'timestamp')
            for key in self.metrics[0].keys()
            if key != 'timestamp'
        }

    def _calculate_average_metrics(self) -> Dict[str, Any]:
        """Calculate average metrics for the session"""
        total = self._calculate_total_metrics()
        sample_count = len(self.metrics)
        return {
            key: value / sample_count
            for key, value in total.items()
        }

    def _calculate_bandwidth_statistics(self) -> Dict[str, float]:
        """Calculate detailed bandwidth statistics"""
        bandwidth_samples = [m['bandwidth_mbps'] for m in self.metrics]
        return {
            'peak_bandwidth_mbps': max(bandwidth_samples),
            'average_bandwidth_mbps': np.mean(bandwidth_samples),
            'median_bandwidth_mbps': np.median(bandwidth_samples),
            'std_bandwidth_mbps': np.std(bandwidth_samples),
            'p95_bandwidth_mbps': np.percentile(bandwidth_samples, 95),
            'p99_bandwidth_mbps': np.percentile(bandwidth_samples, 99)
        }

    def _calculate_error_statistics(self) -> Dict[str, float]:
        """Calculate detailed error statistics"""
        if not self.config.detailed:
            return {}

        error_samples = [m.get('error_rate', 0) for m in self.metrics]
        return {
            'peak_error_rate': max(error_samples),
            'average_error_rate': np.mean(error_samples),
            'total_errors': sum(
                m['errin'] + m['errout'] + m['dropin'] + m['dropout']
                for m in self.metrics
            )
        }

class MemoryMetrics(MetricCollector):
    """Memory metrics collection implementation"""

    def __init__(self, config: ProfilingConfig):
        super().__init__(config)
        self._setup_logging("memory")
        self.task: Optional[asyncio.Task] = None
        self._setup_storage()
        self.process = psutil.Process()

    def _setup_storage(self) -> None:
        """Initialize storage for memory metrics"""
        self.storage_path = Path(self.config.log_directory) / "memory_metrics"
        self.storage_path.mkdir(parents=True, exist_ok=True)

    async def start(self) -> None:
        """Start memory metrics collection"""
        try:
            async with self._async_lock:
                if self.is_collecting:
                    raise MemoryError("Memory metrics collection already running")

                self.start_time = time.time()
                self.is_collecting = True
                self.metrics.clear()

                if self.config.async_mode:
                    self.task = asyncio.create_task(self._collect_metrics_async())
                else:
                    threading.Thread(
                        target=self._collect_metrics_sync,
                        daemon=True
                    ).start()

                self.logger.info("Memory metrics collection started")
                await self._save_state("started")

        except Exception as e:
            self.logger.error(f"Failed to start memory metrics collection: {e}", exc_info=True)
            self.is_collecting = False
            raise MemoryError(f"Failed to start memory collection: {str(e)}") from e

    async def stop(self) -> Dict[str, Any]:
        """Stop memory metrics collection and return results"""
        try:
            async with self._async_lock:
                if not self.is_collecting:
                    raise MemoryError("Memory metrics collection not running")

                self.is_collecting = False
                if self.task:
                    await self.task
                    self.task = None

                results = await self._generate_report()
                await self._save_state("stopped")
                return results

        except Exception as e:
            self.logger.error(f"Error stopping memory metrics collection: {e}", exc_info=True)
            raise MemoryError(f"Failed to stop memory collection: {str(e)}") from e

    async def _collect_metrics_async(self) -> None:
        """Asynchronous memory metrics collection"""
        sample_count = 0

        while self.is_collecting and sample_count < self.config.max_samples:
            try:
                await asyncio.sleep(self.config.interval)
                metrics = await self._get_memory_metrics()
                await self._store_metrics(metrics)

                if await self._should_alert(metrics):
                    await self._send_alert(metrics)

                sample_count += 1

            except Exception as e:
                self.logger.error(f"Error in async memory collection: {e}", exc_info=True)
                await asyncio.sleep(1)

    def _collect_metrics_sync(self) -> None:
        """Synchronous memory metrics collection"""
        sample_count = 0

        while self.is_collecting and sample_count < self.config.max_samples:
            try:
                time.sleep(self.config.interval)
                metrics = self._get_memory_metrics_sync()
                self._store_metrics_sync(metrics)

                if self._should_alert_sync(metrics):
                    self._send_alert_sync(metrics)

                sample_count += 1

            except Exception as e:
                self.logger.error(f"Error in sync memory collection: {e}", exc_info=True)
                time.sleep(1)

    async def _get_memory_metrics(self) -> Dict[str, Any]:
        """Get current memory metrics"""
        process_memory = self.process.memory_info()
        system_memory = psutil.virtual_memory()

        metrics = {
            'timestamp': time.time(),
            'rss': process_memory.rss / (1024 * 1024),  # MB
            'vms': process_memory.vms / (1024 * 1024),  # MB
            'system_total': system_memory.total / (1024 * 1024),  # MB
            'system_available': system_memory.available / (1024 * 1024),  # MB
            'system_percent': system_memory.percent,
            'process_percent': self.process.memory_percent()
        }

        if self.config.detailed:
            metrics.update(await self._get_detailed_memory_metrics())

        return metrics

    async def _get_detailed_memory_metrics(self) -> Dict[str, Any]:
        """Get detailed memory metrics including memory maps"""
        detailed_metrics = {}

        try:
            memory_maps = self.process.memory_maps(grouped=True)
            detailed_metrics.update({
                'shared': sum(m.shared for m in memory_maps) / (1024 * 1024),  # MB
                'private': sum(m.private for m in memory_maps) / (1024 * 1024),  # MB
                'swap': sum(m.swap for m in memory_maps) / (1024 * 1024),  # MB
            })
        except Exception as e:
            self.logger.warning(f"Could not collect detailed memory metrics: {e}")

        return detailed_metrics

    async def _store_metrics(self, metrics: Dict[str, Any]) -> None:
        """Store memory metrics with async file writing"""
        async with self._async_lock:
            self.metrics.append(metrics)

            if self.config.enable_disk_logging:
                file_path = self.storage_path / f"memory_metrics_{int(self.start_time)}.json"
                async with aiofiles.open(file_path, 'a') as f:
                    await f.write(json.dumps(metrics) + '\n')

                if self.config.compression_enabled:
                    await self._compress_old_metrics()

    def _store_metrics_sync(self, metrics: Dict[str, Any]) -> None:
        """Synchronous version of metrics storage"""
        with self._lock:
            self.metrics.append(metrics)

            if self.config.enable_disk_logging:
                file_path = self.storage_path / f"memory_metrics_{int(self.start_time)}.json"
                with open(file_path, 'a') as f:
                    json.dump(metrics, f)
                    f.write('\n')

    async def _should_alert(self, metrics: Dict[str, Any]) -> bool:
        """Check if metrics should trigger an alert"""
        if not self.config.alert_enabled:
            return False

        return (
                metrics['rss'] > self.config.memory_threshold_mb or
                metrics['system_percent'] > 90  # System memory usage above 90%
        )

    async def _send_alert(self, metrics: Dict[str, Any]) -> None:
        """Send alert for concerning metrics"""
        alert_message = (
            f"Memory Alert: Process memory usage {metrics['rss']:.2f} MB "
            f"exceeds threshold {self.config.memory_threshold_mb} MB"
        )
        self.logger.warning(alert_message)
        # Additional alert mechanisms could be added here

    async def _compress_old_metrics(self) -> None:
        """Compress old metric files to save space"""
        import gzip
        import shutil

        for file_path in self.storage_path.glob("*.json"):
            if time.time() - file_path.stat().st_mtime > 86400:  # 24 hours
                gz_path = file_path.with_suffix('.json.gz')
                with open(file_path, 'rb') as f_in:
                    with gzip.open(gz_path, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                file_path.unlink()

    async def _generate_report(self) -> Dict[str, Any]:
        """Generate comprehensive memory metrics report"""
        if not self.metrics:
            return self._generate_empty_report()

        total_metrics = self._calculate_total_metrics()
        peak_metrics = self._calculate_peak_metrics()
        average_metrics = self._calculate_average_metrics()

        report = {
            'start_time': self.start_time,
            'end_time': time.time(),
            'duration': time.time() - self.start_time,
            'total_samples': len(self.metrics),
            'total_metrics': total_metrics,
            'peak_metrics': peak_metrics,
            'average_metrics': average_metrics,
            'memory_statistics': self._calculate_memory_statistics()
        }

        # Save report to disk
        report_path = self.storage_path / f"memory_report_{int(self.start_time)}.json"
        async with aiofiles.open(report_path, 'w') as f:
            await f.write(json.dumps(report, indent=2))

        return report

    def _calculate_memory_statistics(self) -> Dict[str, Any]:
        """Calculate detailed memory statistics"""
        rss_samples = [m['rss'] for m in self.metrics]
        system_percent_samples = [m['system_percent'] for m in self.metrics]

        return {
            'rss_statistics': {
                'peak_mb': max(rss_samples),
                'average_mb': np.mean(rss_samples),
                'median_mb': np.median(rss_samples),
                'std_mb': np.std(rss_samples),
                'p95_mb': np.percentile(rss_samples, 95),
                'p99_mb': np.percentile(rss_samples, 99)
            },
            'system_memory_statistics': {
                'peak_percent': max(system_percent_samples),
                'average_percent': np.mean(system_percent_samples),
                'median_percent': np.median(system_percent_samples),
                'std_percent': np.std(system_percent_samples),
                'time_above_90_percent': len([x for x in system_percent_samples if x > 90])
            }
        }

class ProfilingManager:
    """Main class for managing both network and memory profiling"""

    def __init__(self, config: ProfilingConfig):
        self.config = config
        self.network_metrics = NetworkMetrics(config)
        self.memory_metrics = MemoryMetrics(config)
        self.logger = logging.getLogger(__name__)
        self._setup_logging()

    def _setup_logging(self) -> None:
        """Setup logging for the profiling manager"""
        log_path = Path(self.config.log_directory) / "profiling_manager.log"
        log_path.parent.mkdir(parents=True, exist_ok=True)

        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

        if self.config.enable_disk_logging:
            file_handler = logging.FileHandler(log_path)
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)

        if self.config.enable_console_output:
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)

        self.logger.setLevel(logging.DEBUG)

    async def start_profiling(self, profiling_type: ProfilingType = ProfilingType.BOTH) -> None:
        """Start profiling based on specified type"""
        try:
            if profiling_type in (ProfilingType.NETWORK, ProfilingType.BOTH):
                await self.network_metrics.start()

            if profiling_type in (ProfilingType.MEMORY, ProfilingType.BOTH):
                await self.memory_metrics.start()

            self.logger.info(f"Started profiling with type: {profiling_type}")

        except Exception as e:
            self.logger.error(f"Error starting profiling: {e}", exc_info=True)
            await self.stop_profiling()
            raise ProfilingError(f"Failed to start profiling: {str(e)}")

    async def stop_profiling(self) -> Dict[str, Any]:
        """Stop profiling and return combined results"""
        results = {}

        try:
            if self.network_metrics.is_collecting:
                results['network'] = await self.network_metrics.stop()

            if self.memory_metrics.is_collecting:
                results['memory'] = await self.memory_metrics.stop()

            self.logger.info("Stopped profiling successfully")

            # Generate combined report
            await self._generate_combined_report(results)
            return results

        except Exception as e:
            self.logger.error(f"Error stopping profiling: {e}", exc_info=True)
            raise ProfilingError(f"Failed to stop profiling: {str(e)}")

    async def _generate_combined_report(self, results: Dict[str, Any]) -> None:
        """Generate a combined report with both network and memory metrics"""
        report_path = Path(self.config.log_directory) / "combined_reports"
        report_path.mkdir(parents=True, exist_ok=True)

        combined_report = {
            'timestamp': datetime.now().isoformat(),
            'config': self.config.__dict__,
            'results': results,
            'correlations': await self._calculate_correlations(results)
        }

        filename = report_path / f"combined_report_{int(time.time())}.json"
        async with aiofiles.open(filename, 'w') as f:
            await f.write(json.dumps(combined_report, indent=2))

    async def _calculate_correlations(self, results: Dict[str, Any]) -> Dict[str, float]:
        """Calculate correlations between network and memory metrics"""
        if 'network' not in results or 'memory' not in results:
            return {}

        try:
            network_timestamps = [m['timestamp'] for m in self.network_metrics.metrics]
            memory_timestamps = [m['timestamp'] for m in self.memory_metrics.metrics]

            # Align metrics by timestamp
            aligned_metrics = self._align_metrics_by_time(
                self.network_metrics.metrics,
                self.memory_metrics.metrics
            )

            correlations = {
                'bandwidth_vs_memory': np.corrcoef(
                    [m['bandwidth_mbps'] for m in aligned_metrics['network']],
                    [m['rss'] for m in aligned_metrics['memory']]
                )[0, 1],
                'packets_vs_memory': np.corrcoef(
                    [m['packets_sent'] + m['packets_recv'] for m in aligned_metrics['network']],
                    [m['rss'] for m in aligned_metrics['memory']]
                )[0, 1]
            }

            return correlations

        except Exception as e:
            self.logger.error(f"Error calculating correlations: {e}", exc_info=True)
            return {}

    def _align_metrics_by_time(
            self,
            network_metrics: List[Dict[str, Any]],
            memory_metrics: List[Dict[str, Any]]
    ) -> Dict[str, List[Dict[str, Any]]]:
        """Align network and memory metrics by timestamp"""
        aligned_network = []
        aligned_memory = []

        network_idx = 0
        memory_idx = 0

        while network_idx < len(network_metrics) and memory_idx < len(memory_metrics):
            network_time = network_metrics[network_idx]['timestamp']
            memory_time = memory_metrics[memory_idx]['timestamp']

            if abs(network_time - memory_time) < self.config.interval:
                aligned_network.append(network_metrics[network_idx])
                aligned_memory.append(memory_metrics[memory_idx])
                network_idx += 1
                memory_idx += 1
            elif network_time < memory_time:
                network_idx += 1
            else:
                memory_idx += 1

        return {
            'network': aligned_network,
            'memory': aligned_memory
        }

    @asynccontextmanager
    async def profile(self, profiling_type: ProfilingType = ProfilingType.BOTH):
        """Context manager for easy profiling"""
        try:
            await self.start_profiling(profiling_type)
            yield self
        finally:
            await self.stop_profiling()

    async def get_current_metrics(self) -> Dict[str, Any]:
        """Get current metrics from both network and memory profilers"""
        metrics = {}

        if self.network_metrics.is_collecting:
            metrics['network'] = self.network_metrics.metrics[-1] if self.network_metrics.metrics else {}

        if self.memory_metrics.is_collecting:
            metrics['memory'] = self.memory_metrics.metrics[-1] if self.memory_metrics.metrics else {}

        return metrics

    async def cleanup_old_data(self) -> None:
        """Clean up old metric files based on retention policy"""
        if not self.config.backup_enabled:
            return

        current_time = time.time()
        retention_seconds = self.config.retention_days * 86400

        for path in [self.network_metrics.storage_path, self.memory_metrics.storage_path]:
            for file_path in path.glob("**/*"):
                if file_path.is_file():
                    file_age = current_time - file_path.stat().st_mtime
                    if file_age > retention_seconds:
                        try:
                            file_path.unlink()
                            self.logger.info(f"Deleted old file: {file_path}")
                        except Exception as e:
                            self.logger.error(f"Error deleting old file {file_path}: {e}")

    async def backup_data(self, backup_dir: str) -> None:
        """Backup collected data to specified directory"""
        if not self.config.backup_enabled:
            return

        backup_path = Path(backup_dir)
        backup_path.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        try:
            # Create a compressed archive of all data
            import shutil

            source_dirs = [
                self.network_metrics.storage_path,
                self.memory_metrics.storage_path
            ]

            archive_name = f"profiling_backup_{timestamp}.tar.gz"
            archive_path = backup_path / archive_name

            with tarfile.open(archive_path, "w:gz") as tar:
                for source_dir in source_dirs:
                    tar.add(source_dir, arcname=source_dir.name)

            self.logger.info(f"Backup created successfully: {archive_path}")

        except Exception as e:
            self.logger.error(f"Error creating backup: {e}", exc_info=True)
            raise ProfilingError(f"Failed to create backup: {str(e)}")

    def __repr__(self) -> str:
        return (
            f"ProfilingManager(network_active={self.network_metrics.is_collecting}, "
            f"memory_active={self.memory_metrics.is_collecting})"
        )

# Usage Example
async def main():
    config = ProfilingConfig(
        interval=0.1,
        detailed=True,
        log_directory="profiling_logs",
        max_samples=1000,
        enable_disk_logging=True,
        enable_console_output=True,
        async_mode=True,
        memory_threshold_mb=1000.0,
        network_threshold_mbps=100.0
    )

    profiler = ProfilingManager(config)

    async with profiler.profile(ProfilingType.BOTH) as p:
        # Your application code here
        for _ in range(10):
            metrics = await p.get_current_metrics()
            print(f"Current Metrics: {metrics}")
            await asyncio.sleep(1)

    # Cleanup old data
    await profiler.cleanup_old_data()

    # Backup data
    await profiler.backup_data("backup_directory")

if __name__ == "__main__":
    asyncio.run(main())
Class: ProfilingError
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: profile
  Method: main

Class: ResourceError
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: profile
  Method: main

Class: NetworkError
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: profile
  Method: main

Class: MemoryError
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: profile
  Method: main

Class: ProfilingType
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: profile
  Method: main

Class: ProfilingConfig
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: profile
  Method: main

Class: MetricCollector
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: profile
  Method: main

Class: NetworkMetrics
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: profile
  Method: main

Class: MemoryMetrics
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: profile
  Method: main

Class: ProfilingManager
--------------------------------------------------------------------------------
  Method: __init__
  Method: __init__
  Method: __init__
  Method: __init__
  Method: profile
  Method: main


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\data_collection\resource_lock.py

import threading
from contextlib import contextmanager
import time
import logging
from typing import Optional

class ResourceLock:
    """
    Advanced resource locking mechanism with timeout and deadlock detection.
    """

    def __init__(self, name: str, timeout: float = 5.0):
        self._lock = threading.Lock()
        self._owner: Optional[int] = None
        self.name = name
        self.timeout = timeout
        self.logger = logging.getLogger(__name__)

    @contextmanager
    def __call__(self, timeout: Optional[float] = None):
        acquired = self.acquire(timeout)
        if not acquired:
            raise TimeoutError(f"Failed to acquire lock '{self.name}' within the specified timeout.")
        try:
            yield
        finally:
            self.release()

    def acquire(self, timeout: Optional[float] = None) -> bool:
        """
        Attempt to acquire the lock with a specified timeout.

        Args:
            timeout (Optional[float]): The maximum time to wait for the lock. If None, use the default timeout.

        Returns:
            bool: True if the lock was acquired, False otherwise.
        """
        start_time = time.time()
        timeout = timeout or self.timeout

        while True:
            if self._lock.acquire(blocking=False):
                self._owner = threading.get_ident()
                self.logger.debug(f"Lock '{self.name}' acquired by thread {self._owner}")
                return True

            if time.time() - start_time > timeout:
                self.logger.warning(f"Timeout while attempting to acquire lock '{self.name}'")
                return False

            time.sleep(0.1)

    def release(self) -> None:
        """Release the lock."""
        if self._owner != threading.get_ident():
            raise RuntimeError(f"Attempt to release lock '{self.name}' by non-owner thread")

        self._owner = None
        self._lock.release()
        self.logger.debug(f"Lock '{self.name}' released by thread {threading.get_ident()}")

    def __enter__(self):
        if not self.acquire():
            raise TimeoutError(f"Failed to acquire lock '{self.name}' within the specified timeout.")

    def __exit__(self, exc_type, exc_value, traceback):
        self.release()

    @property
    def locked(self) -> bool:
        """Check if the lock is currently held."""
        return self._lock.locked()

    @property
    def owner(self) -> Optional[int]:
        """Get the ID of the thread that currently holds the lock, if any."""
        return self._owner
Class: ResourceLock
--------------------------------------------------------------------------------
  Method: __init__
  Method: __call__
  Method: __enter__
  Method: __exit__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\data_collection\time_tracking.py

import time
from typing import Dict, Optional, List
import threading
import logging

class TimeTracker:
    """Advanced time tracking for operations with nested timing support."""

    def __init__(self):
        self.start_times: Dict[str, List[float]] = {}
        self.end_times: Dict[str, List[float]] = {}
        self.durations: Dict[str, List[float]] = {}
        self._lock = threading.Lock()
        self.logger = logging.getLogger(__name__)

    def start(self, key: str) -> None:
        """
        Start timing an operation. Supports nested timing for the same key.

        Args:
            key (str): Unique identifier for the operation
        """
        with self._lock:
            if key not in self.start_times:
                self.start_times[key] = []
                self.end_times[key] = []
                self.durations[key] = []
            self.start_times[key].append(time.perf_counter())
        self.logger.debug(f"Started timing for {key}")

    def stop(self, key: str) -> None:
        """
        Stop timing an operation. Matches the most recent start for the key.

        Args:
            key (str): Unique identifier for the operation
        """
        end_time = time.perf_counter()
        with self._lock:
            if key not in self.start_times or not self.start_times[key]:
                raise ValueError(f"No matching start time found for key: {key}")
            start_time = self.start_times[key].pop()
            self.end_times[key].append(end_time)
            duration = end_time - start_time
            self.durations[key].append(duration)
        self.logger.debug(f"Stopped timing for {key}. Duration: {duration:.6f} seconds")

    def get_duration(self, key: str) -> Optional[float]:
        """
        Get the total duration of all timings for a key.

        Args:
            key (str): Unique identifier for the operation

        Returns:
            Optional[float]: Total duration in seconds, or None if not available
        """
        with self._lock:
            if key in self.durations:
                return sum(self.durations[key])
        return None

    def get_average_duration(self, key: str) -> Optional[float]:
        """
        Get the average duration of all timings for a key.

        Args:
            key (str): Unique identifier for the operation

        Returns:
            Optional[float]: Average duration in seconds, or None if not available
        """
        with self._lock:
            if key in self.durations and self.durations[key]:
                return sum(self.durations[key]) / len(self.durations[key])
        return None

    def reset(self) -> None:
        """Reset all timings."""
        with self._lock:
            self.start_times.clear()
            self.end_times.clear()
            self.durations.clear()
        self.logger.info("All timings have been reset")

    def get_all_durations(self) -> Dict[str, List[float]]:
        """
        Get durations for all tracked operations.

        Returns:
            Dict[str, List[float]]: Dictionary of operation keys and their durations
        """
        with self._lock:
            return {key: durations.copy() for key, durations in self.durations.items()}

    def get_summary(self) -> Dict[str, Dict[str, float]]:
        """
        Get a summary of all tracked operations.

        Returns:
            Dict[str, Dict[str, float]]: Dictionary with total, average, min, and max durations for each key
        """
        summary = {}
        with self._lock:
            for key, durations in self.durations.items():
                if durations:
                    summary[key] = {
                        'total': sum(durations),
                        'average': sum(durations) / len(durations),
                        'min': min(durations),
                        'max': max(durations),
                        'count': len(durations)
                    }
        return summary
Class: TimeTracker
--------------------------------------------------------------------------------
  Method: __init__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\data_collection\__init__.py

from .cpu_memory import CPUMemoryTracker
from .gpu_memory import GPUMemoryTracker
from .time_tracking import TimeTracker
from .resource_lock import ResourceLock
from .network_profiler import NetworkProfiler

--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\integration\common_utils.py

from typing import Any, Dict, List
import inspect
import logging

logger = logging.getLogger(__name__)

def identify_framework(model: Any) -> str:
    """
    Identify the deep learning framework of the given model.

    Args:
        model: The machine learning model to identify

    Returns:
        str: The identified framework name ('pytorch', 'tensorflow', 'unknown')
    """
    try:
        model_type = type(model).__module__.split('.')[0]
        if model_type == 'torch':
            return 'pytorch'
        elif model_type in ['tensorflow', 'keras']:
            return 'tensorflow'
        else:
            logger.warning(f"Unknown framework for model type: {model_type}")
            return 'unknown'
    except Exception as e:
        logger.error(f"Error identifying framework: {str(e)}")
        return 'unknown'

def get_model_structure(model: Any) -> Dict[str, Any]:
    """
    Get a structured representation of the model's architecture.

    Args:
        model: The machine learning model

    Returns:
        Dict[str, Any]: A dictionary representing the model's structure
    """
    try:
        framework = identify_framework(model)
        if framework == 'pytorch':
            return _get_pytorch_structure(model)
        elif framework == 'tensorflow':
            return _get_tensorflow_structure(model)
        else:
            return {'error': 'Unsupported framework'}
    except Exception as e:
        logger.error(f"Error getting model structure: {str(e)}")
        return {'error': str(e)}

def _get_pytorch_structure(model: Any) -> Dict[str, Any]:
    """Helper function to get PyTorch model structure."""
    structure = {}
    for name, module in model.named_modules():
        if list(module.children()):  # Skip container modules
            continue
        structure[name] = {
            'type': type(module).__name__,
            'parameters': sum(p.numel() for p in module.parameters()),
            'trainable': sum(p.numel() for p in module.parameters() if p.requires_grad)
        }
    return structure

def _get_tensorflow_structure(model: Any) -> Dict[str, Any]:
    """Helper function to get TensorFlow model structure."""
    structure = {}
    for layer in model.layers:
        structure[layer.name] = {
            'type': type(layer).__name__,
            'parameters': layer.count_params(),
            'trainable': sum(tf.keras.backend.count_params(w) for w in layer.trainable_weights)
        }
    return structure

def estimate_model_size(model: Any) -> Dict[str, float]:
    """
    Estimate the size of the model in memory.

    Args:
        model: The machine learning model

    Returns:
        Dict[str, float]: Estimated size in MB for parameters and buffers
    """
    try:
        framework = identify_framework(model)
        if framework == 'pytorch':
            return _estimate_pytorch_size(model)
        elif framework == 'tensorflow':
            return _estimate_tensorflow_size(model)
        else:
            return {'error': 'Unsupported framework'}
    except Exception as e:
        logger.error(f"Error estimating model size: {str(e)}")
        return {'error': str(e)}

def _estimate_pytorch_size(model: Any) -> Dict[str, float]:
    """Helper function to estimate PyTorch model size."""
    param_size = sum(p.numel() * p.element_size() for p in model.parameters())
    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
    return {
        'parameters': param_size / (1024 * 1024),
        'buffers': buffer_size / (1024 * 1024)
    }

def _estimate_tensorflow_size(model: Any) -> Dict[str, float]:
    """Helper function to estimate TensorFlow model size."""
    param_size = sum(tf.keras.backend.count_params(w) * w.dtype.size for w in model.weights)
    return {
        'parameters': param_size / (1024 * 1024),
        'buffers': 0  # TensorFlow doesn't have a direct equivalent to PyTorch's buffers
    }

def get_function_info(func: callable) -> Dict[str, Any]:
    """
    Get detailed information about a function.

    Args:
        func: The function to inspect

    Returns:
        Dict[str, Any]: Information about the function
    """
    try:
        signature = inspect.signature(func)
        return {
            'name': func.__name__,
            'module': func.__module__,
            'docstring': inspect.getdoc(func),
            'parameters': [{'name': name, 'annotation': param.annotation.__name__ if param.annotation != inspect.Parameter.empty else None}
                           for name, param in signature.parameters.items()],
            'return_annotation': signature.return_annotation.__name__ if signature.return_annotation != inspect.Signature.empty else None,
            'is_coroutine': inspect.iscoroutinefunction(func),
            'is_generator': inspect.isgeneratorfunction(func),
            'source': inspect.getsource(func)
        }
    except Exception as e:
        logger.error(f"Error getting function info: {str(e)}")
        return {'error': str(e)}

--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\integration\framework_adapter.py

# memoraith/integration/framework_adapter.py
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import logging
import asyncio
from ..exceptions import MemoraithError

class FrameworkAdapter(ABC):
    """Base class for framework-specific adapters."""

    def __init__(self, model: Any):
        self.model = model
        self.data: Dict[str, Any] = {}
        self.logger = logging.getLogger(__name__)
        self._is_profiling = False
        self._lock = asyncio.Lock()

    async def __aenter__(self):
        """Context manager entry."""
        await self.start_profiling()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        await self.stop_profiling()
        if exc_type is not None:
            self.logger.error(f"Error during profiling: {exc_val}")
            return False
        return True

    @abstractmethod
    async def start_profiling(self) -> None:
        """Start profiling the model."""
        raise NotImplementedError

    @abstractmethod
    async def stop_profiling(self) -> None:
        """Stop profiling the model."""
        raise NotImplementedError

    @abstractmethod
    async def profile_inference(self, input_data: Any) -> Dict[str, Any]:
        """Profile model inference."""
        raise NotImplementedError

    @abstractmethod
    async def profile_training_step(self, input_data: Any, target: Any) -> Dict[str, Any]:
        """Profile a single training step."""
        raise NotImplementedError

    async def get_profiling_data(self) -> Dict[str, Any]:
        """Get collected profiling data."""
        async with self._lock:
            return self.data.copy()

    @abstractmethod
    async def get_model_summary(self) -> Dict[str, Any]:
        """Get model architecture summary."""
        raise NotImplementedError

    async def validate_model(self) -> bool:
        """Validate model compatibility."""
        try:
            # Basic validation
            if self.model is None:
                raise MemoraithError("Model is None")

            # Framework-specific validation should be implemented
            # in derived classes
            return True
        except Exception as e:
            self.logger.error(f"Model validation failed: {str(e)}")
            return False

    @abstractmethod
    async def cleanup(self) -> None:
        """Clean up resources."""
        raise NotImplementedError
Class: FrameworkAdapter
--------------------------------------------------------------------------------
  Method: __init__
  Method: __aenter__
  Method: __aexit__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\integration\pytorch_adapter.py

import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.cuda.nvtx as nvtx
import psutil
import threading
import time
import logging
from typing import Dict, Any, List, Optional, Tuple, Set
from enum import Enum
from dataclasses import dataclass
import numpy as np
from pathlib import Path
import json
import queue
from collections import defaultdict

class ProfilingLevel(Enum):
    BASIC = "basic"          # Basic metrics only
    MEMORY = "memory"        # Memory-focused profiling
    COMPUTE = "compute"      # Computation-focused profiling
    FULL = "full"           # All metrics and features

@dataclass
class LayerProfile:
    """Detailed layer profiling information"""
    name: str
    layer_type: str
    input_shape: Tuple
    output_shape: Tuple
    parameters: int
    cpu_memory: float
    gpu_memory: float
    compute_time: float
    flops: int
    backward_time: float
    gradient_norm: float
    activation_memory: float
    buffer_memory: float
    cuda_memory_allocated: float
    cuda_memory_cached: float
    cuda_utilization: float
    peak_memory: float

class PyTorchAdapter:
    """Advanced PyTorch model profiling adapter"""

    def __init__(
            self,
            model: nn.Module,
            level: ProfilingLevel = ProfilingLevel.FULL,
            log_dir: str = "profiling_logs",
            device: Optional[torch.device] = None
    ):
        self.model = model
        self.level = level
        self.log_dir = Path(log_dir)
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Setup logging
        self.logger = logging.getLogger(__name__)
        self._setup_logging()

        # Profiling data structures
        self.layer_profiles: Dict[str, LayerProfile] = {}
        self.training_history: List[Dict[str, float]] = []
        self.memory_traces: Dict[str, List[float]] = defaultdict(list)
        self.gradient_history: Dict[str, List[float]] = defaultdict(list)
        self.activation_maps: Dict[str, List[torch.Tensor]] = {}
        self.bottlenecks: Set[str] = set()

        # Performance monitoring
        self._monitoring_queue = queue.Queue()
        self._stop_monitoring = threading.Event()
        self._monitoring_thread: Optional[threading.Thread] = None

        # CUDA events for precise timing
        self.cuda_events: Dict[str, torch.cuda.Event] = {}

        # Move model to device
        self.model.to(self.device)
        self._attach_hooks()
        self.logger.info(f"PyTorch adapter initialized with profiling level: {level.value}")

    def _setup_logging(self) -> None:
        """Configure detailed logging"""
        self.log_dir.mkdir(parents=True, exist_ok=True)
        fh = logging.FileHandler(self.log_dir / "pytorch_profiling.log")
        fh.setLevel(logging.DEBUG)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        fh.setFormatter(formatter)
        self.logger.addHandler(fh)
        self.logger.setLevel(logging.DEBUG)

    def _attach_hooks(self) -> None:
        """Attach comprehensive profiling hooks to model"""
        for name, module in self.model.named_modules():
            # Forward pre-hook for input analysis
            module.register_forward_pre_hook(self._forward_pre_hook(name))

            # Forward hook for output analysis
            module.register_forward_hook(self._forward_hook(name))

            # Backward hook for gradient analysis
            if hasattr(module, 'weight') and module.weight is not None:
                module.register_full_backward_hook(self._backward_hook(name))

            self.cuda_events[f"{name}_forward_start"] = torch.cuda.Event(enable_timing=True)
            self.cuda_events[f"{name}_forward_end"] = torch.cuda.Event(enable_timing=True)
            self.cuda_events[f"{name}_backward_start"] = torch.cuda.Event(enable_timing=True)
            self.cuda_events[f"{name}_backward_end"] = torch.cuda.Event(enable_timing=True)

    def _forward_pre_hook(self, name: str):
        """Pre-forward pass hook for input analysis"""
        def hook(module: nn.Module, input: Tuple[torch.Tensor]):
            if self.level != ProfilingLevel.BASIC:
                try:
                    # Start CUDA timing
                    event = self.cuda_events[f"{name}_forward_start"]
                    event.record()

                    # Record input statistics
                    if input[0].requires_grad:
                        input_size = input[0].element_size() * input[0].nelement()
                        self.memory_traces[f"{name}_input"].append(input_size)

                    # CUDA memory tracking
                    if torch.cuda.is_available():
                        self._record_cuda_memory(name, "pre_forward")

                except Exception as e:
                    self.logger.error(f"Error in forward pre-hook for {name}: {str(e)}")
        return hook

    def _forward_hook(self, name: str):
        """Post-forward pass hook for comprehensive analysis"""
        def hook(module: nn.Module, input: Tuple[torch.Tensor], output: torch.Tensor):
            try:
                # End CUDA timing
                event_start = self.cuda_events[f"{name}_forward_start"]
                event_end = self.cuda_events[f"{name}_forward_end"]
                event_end.record()

                # Basic metrics
                profile = LayerProfile(
                    name=name,
                    layer_type=module.__class__.__name__,
                    input_shape=tuple(input[0].shape),
                    output_shape=tuple(output.shape) if isinstance(output, torch.Tensor) else None,
                    parameters=sum(p.numel() for p in module.parameters() if p.requires_grad),
                    cpu_memory=0,
                    gpu_memory=0,
                    compute_time=0,
                    flops=self._calculate_flops(module, input[0], output),
                    backward_time=0,
                    gradient_norm=0,
                    activation_memory=0,
                    buffer_memory=0,
                    cuda_memory_allocated=0,
                    cuda_memory_cached=0,
                    cuda_utilization=0,
                    peak_memory=0
                )

                if self.level in (ProfilingLevel.MEMORY, ProfilingLevel.FULL):
                    # Memory analysis
                    self._analyze_memory(name, module, output, profile)

                if self.level in (ProfilingLevel.COMPUTE, ProfilingLevel.FULL):
                    # Computation analysis
                    self._analyze_computation(name, module, profile, event_start, event_end)

                # Store activation maps for visualization
                if isinstance(output, torch.Tensor):
                    self.activation_maps[name] = output.detach().cpu().numpy()

                # Update profile
                self.layer_profiles[name] = profile

                # Check for bottlenecks
                self._check_bottlenecks(name, profile)

            except Exception as e:
                self.logger.error(f"Error in forward hook for {name}: {str(e)}")
        return hook

    def _backward_hook(self, name: str):
        """Backward pass hook for gradient analysis"""
        def hook(module: nn.Module, grad_input: Tuple[torch.Tensor], grad_output: Tuple[torch.Tensor]):
            try:
                # Record gradient timing
                event_start = self.cuda_events[f"{name}_backward_start"]
                event_end = self.cuda_events[f"{name}_backward_end"]
                event_start.record()

                if self.level in (ProfilingLevel.COMPUTE, ProfilingLevel.FULL):
                    # Gradient analysis
                    if grad_output[0] is not None:
                        grad_norm = grad_output[0].norm().item()
                        self.gradient_history[name].append(grad_norm)

                        if name in self.layer_profiles:
                            self.layer_profiles[name].gradient_norm = grad_norm

                event_end.record()

            except Exception as e:
                self.logger.error(f"Error in backward hook for {name}: {str(e)}")
        return hook

    def _analyze_memory(self, name: str, module: nn.Module, output: torch.Tensor, profile: LayerProfile) -> None:
        """Detailed memory analysis for a layer"""
        try:
            # CPU memory
            profile.cpu_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

            # GPU memory if available
            if torch.cuda.is_available():
                profile.gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                profile.cuda_memory_allocated = torch.cuda.memory_allocated() / 1024 / 1024
                profile.cuda_memory_cached = torch.cuda.memory_reserved() / 1024 / 1024

                # Peak memory tracking
                profile.peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024

            # Activation memory
            if isinstance(output, torch.Tensor):
                profile.activation_memory = output.element_size() * output.nelement() / 1024 / 1024

            # Buffer memory
            profile.buffer_memory = sum(b.element_size() * b.nelement() for b in module.buffers()) / 1024 / 1024

        except Exception as e:
            self.logger.error(f"Error in memory analysis for {name}: {str(e)}")

    def _analyze_computation(self, name: str, module: nn.Module, profile: LayerProfile,
                             event_start: torch.cuda.Event, event_end: torch.cuda.Event) -> None:
        """Detailed computation analysis for a layer"""
        try:
            # CUDA timing
            torch.cuda.synchronize()
            profile.compute_time = event_start.elapsed_time(event_end) / 1000  # Convert to seconds

            # CUDA utilization
            if torch.cuda.is_available():
                profile.cuda_utilization = torch.cuda.utilization()

        except Exception as e:
            self.logger.error(f"Error in computation analysis for {name}: {str(e)}")

    def _calculate_flops(self, module: nn.Module, input_tensor: torch.Tensor,
                         output_tensor: torch.Tensor) -> int:
        """Calculate FLOPs for different layer types"""
        try:
            if isinstance(module, nn.Conv2d):
                return self._conv2d_flops(module, input_tensor)
            elif isinstance(module, nn.Linear):
                return self._linear_flops(module)
            elif isinstance(module, nn.LSTM):
                return self._lstm_flops(module)
            return 0
        except Exception as e:
            self.logger.error(f"Error calculating FLOPs: {str(e)}")
            return 0

    def _conv2d_flops(self, module: nn.Conv2d, input_tensor: torch.Tensor) -> int:
        """Calculate FLOPs for Conv2d layer"""
        batch_size = input_tensor.shape[0]
        output_height = input_tensor.shape[2] // module.stride[0]
        output_width = input_tensor.shape[3] // module.stride[1]

        kernel_ops = module.kernel_size[0] * module.kernel_size[1] * module.in_channels
        flops_per_instance = kernel_ops * module.out_channels
        total_flops = flops_per_instance * output_height * output_width * batch_size

        return total_flops

    def _check_bottlenecks(self, name: str, profile: LayerProfile) -> None:
        """Identify performance bottlenecks"""
        try:
            # Memory bottleneck
            if profile.gpu_memory > 1000:  # More than 1GB
                self.bottlenecks.add(f"{name}_memory")

            # Compute bottleneck
            if profile.compute_time > 0.1:  # More than 100ms
                self.bottlenecks.add(f"{name}_compute")

            # Gradient bottleneck
            if profile.gradient_norm > 100:
                self.bottlenecks.add(f"{name}_gradient")

        except Exception as e:
            self.logger.error(f"Error checking bottlenecks for {name}: {str(e)}")

    def start_monitoring(self) -> None:
        """Start continuous resource monitoring"""
        self._stop_monitoring.clear()
        self._monitoring_thread = threading.Thread(target=self._monitor_resources)
        self._monitoring_thread.daemon = True
        self._monitoring_thread.start()

    def stop_monitoring(self) -> None:
        """Stop resource monitoring"""
        self._stop_monitoring.set()
        if self._monitoring_thread:
            self._monitoring_thread.join()

    def _monitor_resources(self) -> None:
        """Continuous resource monitoring thread"""
        while not self._stop_monitoring.is_set():
            try:
                gpu_info = {
                    'memory_allocated': torch.cuda.memory_allocated() / 1024 / 1024,
                    'memory_cached': torch.cuda.memory_reserved() / 1024 / 1024,
                    'utilization': torch.cuda.utilization()
                } if torch.cuda.is_available() else {}

                cpu_info = {
                    'cpu_percent': psutil.cpu_percent(),
                    'memory_percent': psutil.virtual_memory().percent,
                    'swap_percent': psutil.swap_memory().percent
                }

                self._monitoring_queue.put({
                    'timestamp': time.time(),
                    'gpu': gpu_info,
                    'cpu': cpu_info
                })

                time.sleep(0.1)  # 100ms interval

            except Exception as e:
                self.logger.error(f"Error in resource monitoring: {str(e)}")

    def get_profiling_results(self) -> Dict[str, Any]:
        """Get comprehensive profiling results"""
        return {
            'layer_profiles': {name: vars(profile) for name, profile in self.layer_profiles.items()},
            'bottlenecks': list(self.bottlenecks),
            'training_history': self.training_history,
            'gradient_history': dict(self.gradient_history),
            'memory_traces': dict(self.memory_traces),
            'monitoring_data': self._get_monitoring_data()
        }


    def _get_monitoring_data(self) -> List[Dict[str, Any]]:
        """Get collected monitoring data"""
        data = []
        while not self._monitoring_queue.empty():
            data.append(self._monitoring_queue.get())
        return data


    def save_results(self, filename: str) -> None:
        """Save profiling results to file"""
        try:
            results = self.get_profiling_results()

            # Convert numpy arrays to lists for JSON serialization
            for name, activation in self.activation_maps.items():
                if isinstance(activation, np.ndarray):
                    results[f'activation_{name}'] = activation.tolist()

            # Save to file
            with open(filename, 'w') as f:
                json.dump(results, f, indent=4)

            self.logger.info(f"Profiling results saved to {filename}")

        except Exception as e:
            self.logger.error(f"Error saving profiling results: {str(e)}")
            raise

    async def profile_training_step(self,
                                    input_data: torch.Tensor,
                                    target: torch.Tensor,
                                    optimizer: torch.optim.Optimizer,
                                    criterion: nn.Module) -> Dict[str, float]:
        """Profile a single training step with detailed metrics"""
        try:
            # Start timing
            start_event = torch.cuda.Event(enable_timing=True)
            end_event = torch.cuda.Event(enable_timing=True)
            start_event.record()

            # Clear gradients
            optimizer.zero_grad()

            # Forward pass timing
            forward_start = torch.cuda.Event(enable_timing=True)
            forward_end = torch.cuda.Event(enable_timing=True)

            forward_start.record()
            output = self.model(input_data.to(self.device))
            forward_end.record()

            # Loss computation
            loss = criterion(output, target.to(self.device))

            # Backward pass timing
            backward_start = torch.cuda.Event(enable_timing=True)
            backward_end = torch.cuda.Event(enable_timing=True)

            backward_start.record()
            loss.backward()
            backward_end.record()

            # Optimizer step timing
            optim_start = torch.cuda.Event(enable_timing=True)
            optim_end = torch.cuda.Event(enable_timing=True)

            optim_start.record()
            optimizer.step()
            optim_end.record()

            # Synchronize and record timings
            torch.cuda.synchronize()

            metrics = {
                'total_time': start_event.elapsed_time(end_event) / 1000,
                'forward_time': forward_start.elapsed_time(forward_end) / 1000,
                'backward_time': backward_start.elapsed_time(backward_end) / 1000,
                'optimizer_time': optim_start.elapsed_time(optim_end) / 1000,
                'loss': loss.item(),
                'gpu_memory': torch.cuda.memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0,
                'cpu_memory': psutil.Process().memory_info().rss / 1024 / 1024
            }

            # Record gradient norms
            grad_norms = {
                name: param.grad.norm().item()
                for name, param in self.model.named_parameters()
                if param.grad is not None
            }
            metrics['gradient_norms'] = grad_norms

            # Add to training history
            self.training_history.append(metrics)

            return metrics

        except Exception as e:
            self.logger.error(f"Error in training step profiling: {str(e)}")
            raise

    def analyze_model_architecture(self) -> Dict[str, Any]:
        """Analyze model architecture for optimization opportunities"""
        try:
            analysis = {
                'total_parameters': sum(p.numel() for p in self.model.parameters()),
                'trainable_parameters': sum(p.numel() for p in self.model.parameters() if p.requires_grad),
                'layer_distribution': {},
                'memory_distribution': {},
                'compute_distribution': {},
                'optimization_suggestions': []
            }

            # Analyze layer distribution
            layer_types = {}
            for name, module in self.model.named_modules():
                layer_type = module.__class__.__name__
                layer_types[layer_type] = layer_types.get(layer_type, 0) + 1
            analysis['layer_distribution'] = layer_types

            # Memory and compute analysis
            total_memory = sum(p.memory_usage for p in self.layer_profiles.values())
            total_compute = sum(p.compute_time for p in self.layer_profiles.values())

            for name, profile in self.layer_profiles.items():
                analysis['memory_distribution'][name] = profile.memory_usage / total_memory
                analysis['compute_distribution'][name] = profile.compute_time / total_compute

            # Generate optimization suggestions
            self._generate_optimization_suggestions(analysis)

            return analysis

        except Exception as e:
            self.logger.error(f"Error in model architecture analysis: {str(e)}")
            raise

    def _generate_optimization_suggestions(self, analysis: Dict[str, Any]) -> None:
        """Generate optimization suggestions based on profiling data"""
        suggestions = []

        # Memory optimization suggestions
        for name, memory_percent in analysis['memory_distribution'].items():
            if memory_percent > 0.2:  # Layer uses more than 20% of total memory
                suggestions.append({
                    'type': 'memory',
                    'layer': name,
                    'suggestion': 'Consider reducing layer size or using gradient checkpointing',
                    'impact': 'high'
                })

        # Compute optimization suggestions
        for name, compute_percent in analysis['compute_distribution'].items():
            if compute_percent > 0.3:  # Layer uses more than 30% of compute time
                suggestions.append({
                    'type': 'compute',
                    'layer': name,
                    'suggestion': 'Consider using a more efficient layer type or reducing complexity',
                    'impact': 'high'
                })

        # Model-wide suggestions
        if analysis['total_parameters'] > 1e8:  # More than 100M parameters
            suggestions.append({
                'type': 'model',
                'suggestion': 'Consider model pruning or quantization',
                'impact': 'medium'
            })

        analysis['optimization_suggestions'] = suggestions

    def visualize_profiling_results(self, output_dir: str) -> None:
        """Generate comprehensive visualization of profiling results"""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns

            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)

            # Memory usage over time
            plt.figure(figsize=(12, 6))
            plt.plot(self.training_history)
            plt.title('Memory Usage Over Time')
            plt.xlabel('Training Step')
            plt.ylabel('Memory (MB)')
            plt.savefig(output_path / 'memory_usage.png')
            plt.close()

            # Layer compute times
            compute_times = {name: profile.compute_time for name, profile in self.layer_profiles.items()}
            plt.figure(figsize=(15, 8))
            sns.barplot(x=list(compute_times.values()), y=list(compute_times.keys()))
            plt.title('Layer Compute Times')
            plt.xlabel('Time (s)')
            plt.savefig(output_path / 'compute_times.png')
            plt.close()

            # Generate HTML report
            self._generate_html_report(output_path)

        except Exception as e:
            self.logger.error(f"Error in visualization: {str(e)}")
            raise

    def _generate_html_report(self, output_path: Path) -> None:
        """Generate detailed HTML report with all profiling information"""
        try:
            analysis = self.analyze_model_architecture()
            results = self.get_profiling_results()

            html_content = f"""
            <html>
                <head>
                    <title>PyTorch Model Profiling Report</title>
                    <style>
                        body {{ font-family: Arial, sans-serif; margin: 20px; }}
                        .section {{ margin: 20px 0; padding: 20px; border: 1px solid #ddd; }}
                        .warning {{ color: red; }}
                        .metric {{ font-weight: bold; }}
                    </style>
                </head>
                <body>
                    <h1>PyTorch Model Profiling Report</h1>
                    
                    <div class="section">
                        <h2>Model Architecture</h2>
                        <p>Total Parameters: {analysis['total_parameters']:,}</p>
                        <p>Trainable Parameters: {analysis['trainable_parameters']:,}</p>
                    </div>

                    <div class="section">
                        <h2>Performance Metrics</h2>
                        <p>Peak GPU Memory: {max(p.gpu_memory for p in self.layer_profiles.values()):.2f} MB</p>
                        <p>Total Compute Time: {sum(p.compute_time for p in self.layer_profiles.values()):.2f} s</p>
                    </div>

                    <div class="section">
                        <h2>Optimization Suggestions</h2>
                        {''.join(f'<p class="warning">{s["suggestion"]}</p>' for s in analysis['optimization_suggestions'])}
                    </div>

                    <div class="section">
                        <h2>Layer Profiles</h2>
                        {''.join(f'<div class="metric">{name}: {profile.compute_time:.4f}s, {profile.gpu_memory:.2f}MB</div>'
                                 for name, profile in self.layer_profiles.items())}
                    </div>
                </body>
            </html>
            """

            with open(output_path / 'profiling_report.html', 'w') as f:
                f.write(html_content)

        except Exception as e:
            self.logger.error(f"Error generating HTML report: {str(e)}")
            raise

    def __enter__(self):
        """Context manager entry"""
        self.start_monitoring()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        self.stop_monitoring()

Class: ProfilingLevel
--------------------------------------------------------------------------------
  Method: _forward_pre_hook
  Method: hook
  Method: _forward_hook
  Method: hook
  Method: _backward_hook
  Method: hook
  Method: __enter__
  Method: __exit__

Class: LayerProfile
--------------------------------------------------------------------------------
  Method: _forward_pre_hook
  Method: hook
  Method: _forward_hook
  Method: hook
  Method: _backward_hook
  Method: hook
  Method: __enter__
  Method: __exit__

Class: PyTorchAdapter
--------------------------------------------------------------------------------
  Method: _forward_pre_hook
  Method: hook
  Method: _forward_hook
  Method: hook
  Method: _backward_hook
  Method: hook
  Method: __enter__
  Method: __exit__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\integration\tensorflow_adapter.py

import tensorflow as tf
import time
from typing import Dict, Any, List
import logging
from .framework_adapter import FrameworkAdapter
from ..data_collection import TimeTracker, CPUMemoryTracker, GPUMemoryTracker
from ..config import config

class TensorFlowAdapter(FrameworkAdapter):
    """Advanced adapter for integrating with TensorFlow models."""

    def __init__(self, model: tf.keras.Model):
        super().__init__(model)
        self.time_tracker = TimeTracker()
        self.cpu_tracker = CPUMemoryTracker()
        self.gpu_tracker = GPUMemoryTracker() if tf.test.is_built_with_cuda() and config.enable_gpu else None
        self.original_call = None
        self.logger = logging.getLogger(__name__)

    async def start_profiling(self) -> None:
        """Start profiling by wrapping the model's call method."""
        self.original_call = self.model.call
        self.model.call = self._wrapped_call

        await self.cpu_tracker.start()
        if self.gpu_tracker:
            await self.gpu_tracker.start()
        await self.log_profiling_start()

    async def stop_profiling(self) -> None:
        """Restore the original call method after profiling."""
        if self.original_call:
            self.model.call = self.original_call
            self.original_call = None

        await self.cpu_tracker.stop()
        if self.gpu_tracker:
            await self.gpu_tracker.stop()
        await self.log_profiling_stop()

    async def _wrapped_call(self, *args, **kwargs):
        """Wrapped call method for profiling each layer."""
        for layer in self.model.layers:
            layer_name = f"{layer.__class__.__name__}_{id(layer)}"
            self.time_tracker.start(layer_name)
            output = layer(*args, **kwargs)
            self.time_tracker.stop(layer_name)

            try:
                self.data[layer_name] = self.data.get(layer_name, {})
                self.data[layer_name]['time'] = self.time_tracker.get_duration(layer_name)
                self.data[layer_name]['parameters'] = layer.count_params()

                if config.enable_memory:
                    self.data[layer_name]['cpu_memory'] = await self.cpu_tracker.get_peak_memory()
                    if self.gpu_tracker:
                        self.data[layer_name]['gpu_memory'] = await self.gpu_tracker.get_peak_memory()

                if hasattr(output, 'shape'):
                    self.data[layer_name]['output_shape'] = output.shape.as_list()
            except Exception as e:
                self.logger.error(f"Error in _wrapped_call for layer {layer_name}: {str(e)}")

            args = (output,)

        return await self.original_call(*args, **kwargs)

    async def profile_inference(self, input_data: tf.Tensor) -> Dict[str, Any]:
        """Profile the inference process for a single input."""
        await self.start_profiling()
        try:
            start_time = time.perf_counter()
            output = self.model(input_data)
            end_time = time.perf_counter()

            inference_time = end_time - start_time
            profiling_data = self.data.copy()
            profiling_data['inference_time'] = inference_time

            return profiling_data
        finally:
            await self.stop_profiling()

    async def profile_training_step(self, input_data: tf.Tensor, target: tf.Tensor) -> Dict[str, Any]:
        """Profile a single training step."""
        await self.start_profiling()
        try:
            optimizer = self.model.optimizer
            loss_function = self.model.loss

            if optimizer is None or loss_function is None:
                raise ValueError("Optimizer or loss function not properly configured")

            start_time = time.perf_counter()
            with tf.GradientTape() as tape:
                predictions = self.model(input_data, training=True)
                loss = loss_function(target, predictions)
            gradients = tape.gradient(loss, self.model.trainable_variables)
            optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
            end_time = time.perf_counter()

            step_time = end_time - start_time
            profiling_data = self.data.copy()
            profiling_data['step_time'] = step_time
            profiling_data['loss'] = loss.numpy().item()

            return profiling_data
        finally:
            await self.stop_profiling()

    async def get_model_summary(self) -> Dict[str, Any]:
        """Get a summary of the model architecture."""
        summary = {}
        total_params = self.model.count_params()
        trainable_params = sum(tf.keras.backend.count_params(w) for w in self.model.trainable_weights)

        summary['total_params'] = total_params
        summary['trainable_params'] = trainable_params
        summary['non_trainable_params'] = total_params - trainable_params
        summary['model_size_mb'] = total_params * 4 / (1024 * 1024)  # Assuming float32

        return summary

    async def get_layer_info(self, layer_name: str) -> Dict[str, Any]:
        """Get detailed information about a specific layer."""
        for layer in self.model.layers:
            if layer.name == layer_name:
                return {
                    'type': type(layer).__name__,
                    'parameters': layer.count_params(),
                    'trainable_parameters': sum(tf.keras.backend.count_params(w) for w in layer.trainable_weights),
                    'input_shape': layer.input_shape,
                    'output_shape': layer.output_shape,
                    'activation': layer.activation.__name__ if hasattr(layer, 'activation') else None,
                }
        return {}

    async def profile_memory_usage(self) -> Dict[str, float]:
        """Profile the memory usage of the model."""
        tf.keras.backend.clear_session()

        initial_mem = tf.config.experimental.get_memory_info('GPU:0')['current'] if tf.test.is_gpu_available() else 0

        # Profile memory usage during forward pass
        dummy_input = tf.random.normal(shape=[1] + self.model.input_shape[1:])
        self.model(dummy_input)
        forward_mem = tf.config.experimental.get_memory_info('GPU:0')['current'] if tf.test.is_gpu_available() else 0

        # Profile memory usage during backward pass
        with tf.GradientTape() as tape:
            predictions = self.model(dummy_input)
            loss = tf.reduce_mean(predictions)
        _ = tape.gradient(loss, self.model.trainable_variables)
        backward_mem = tf.config.experimental.get_memory_info('GPU:0')['current'] if tf.test.is_gpu_available() else 0

        return {
            'initial_memory': initial_mem / (1024 * 1024),  # Convert to MB
            'forward_pass_memory': (forward_mem - initial_mem) / (1024 * 1024),
            'backward_pass_memory': (backward_mem - forward_mem) / (1024 * 1024),
            'total_memory': (backward_mem - initial_mem) / (1024 * 1024)
        }

    def get_flops(self) -> int:
        """Calculate the total number of FLOPs for the model."""
        run_meta = tf.compat.v1.RunMetadata()
        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()
        flops = tf.compat.v1.profiler.profile(tf.compat.v1.get_default_graph(), run_meta=run_meta, cmd='scope', options=opts)
        return flops.total_float_ops

    async def get_current_time(self) -> float:
        """Get the current time in a high-precision format."""
        return time.perf_counter()

    async def export_model(self, path: str, format: str) -> None:
        """Export the model to a specified format."""
        if format.lower() == 'savedmodel':
            tf.saved_model.save(self.model, path)
        elif format.lower() == 'h5':
            self.model.save(path, save_format='h5')
        else:
            raise ValueError(f"Unsupported export format: {format}")
        self.logger.info(f"Model exported to {format} format at {path}")

    async def visualize_model(self, output_path: str) -> None:
        """Generate a visual representation of the model architecture."""
        tf.keras.utils.plot_model(self.model, to_file=output_path, show_shapes=True, show_layer_names=True)
        self.logger.info(f"Model visualization saved to {output_path}")

    async def get_optimizer_info(self) -> Dict[str, Any]:
        """Get information about the current optimizer."""
        optimizer = self.model.optimizer
        return {
            'name': optimizer.__class__.__name__,
            'learning_rate': float(tf.keras.backend.get_value(optimizer.lr)),
            'parameters': {k: v.numpy() for k, v in optimizer.get_config().items() if k != 'name'}
        }

    async def get_loss_function_info(self) -> Dict[str, Any]:
        """Get information about the current loss function."""
        loss = self.model.loss
        if callable(loss):
            return {'name': loss.__name__}
        elif isinstance(loss, str):
            return {'name': loss}
        elif isinstance(loss, tf.keras.losses.Loss):
            return {
                'name': loss.__class__.__name__,
                'parameters': loss.get_config()
            }
        else:
            return {'error': 'Unknown loss type'}
Class: TensorFlowAdapter
--------------------------------------------------------------------------------
  Method: __init__
  Method: _wrapped_call


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\integration\__init__.py

from .framework_adapter import FrameworkAdapter
from .pytorch_adapter import PyTorchAdapter
from .tensorflow_adapter import TensorFlowAdapter
from .common_utils import identify_framework

def get_framework_adapter(model):
    framework_name = identify_framework(model)
    if framework_name == 'pytorch':
        return PyTorchAdapter(model)
    elif framework_name == 'tensorflow':
        return TensorFlowAdapter(model)
    else:
        from memoraith.exceptions import FrameworkNotSupportedError
        raise FrameworkNotSupportedError(framework_name)

--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\reporting\console_report.py

from typing import Dict, Any
import logging
from colorama import Fore, Style, init

init(autoreset=True)  # Initialize colorama

class ConsoleReport:
    """Generates a comprehensive console report from the analysis results."""

    def __init__(self, analysis_results: Dict[str, Any]):
        self.analysis_results = analysis_results
        self.logger = logging.getLogger(__name__)

    def display(self) -> None:
        """Display the report in the console with enhanced formatting and colors."""
        try:
            print(f"\n{Fore.CYAN}{Style.BRIGHT}==== Memoraith Profiling Report ===={Style.RESET_ALL}")

            self._display_global_metrics()
            self._display_top_consumers('time', 'Time Consumers', 'total_time', 's')
            self._display_top_consumers('cpu_memory', 'CPU Memory Consumers', 'total_cpu_memory', 'MB')
            self._display_top_consumers('gpu_memory', 'GPU Memory Consumers', 'total_gpu_memory', 'MB')
            self._display_bottlenecks()
            self._display_recommendations()
            self._display_anomalies()

            print(f"\n{Fore.YELLOW}For detailed visualizations and interactive dashboard, please refer to the generated HTML report.")
        except Exception as e:
            self.logger.error(f"Error displaying console report: {str(e)}")

    def _display_global_metrics(self) -> None:
        """Display global metrics."""
        print(f"\n{Fore.GREEN}{Style.BRIGHT}Global Metrics:{Style.RESET_ALL}")
        global_metrics = self.analysis_results['metrics'].get('global', {})
        print(f"Total Time: {Fore.YELLOW}{global_metrics.get('total_time', 0):.4f} s{Style.RESET_ALL}")
        print(f"Peak CPU Memory: {Fore.YELLOW}{global_metrics.get('peak_cpu_memory', 0):.2f} MB{Style.RESET_ALL}")
        print(f"Peak GPU Memory: {Fore.YELLOW}{global_metrics.get('peak_gpu_memory', 0):.2f} MB{Style.RESET_ALL}")
        print(f"Total Parameters: {Fore.YELLOW}{global_metrics.get('total_parameters', 0):,}{Style.RESET_ALL}")
        if 'total_flops' in global_metrics:
            print(f"Total FLOPs: {Fore.YELLOW}{global_metrics['total_flops']:,}{Style.RESET_ALL}")

    def _display_top_consumers(self, metric_type: str, title: str, metric_key: str, unit: str, top_n: int = 5) -> None:
        """Display top consumers for a specific metric."""
        print(f"\n{Fore.GREEN}{Style.BRIGHT}Top {top_n} {title}:{Style.RESET_ALL}")
        sorted_layers = sorted(
            [(layer, metrics[metric_key]) for layer, metrics in self.analysis_results['metrics'].items() if isinstance(metrics, dict)],
            key=lambda x: x[1],
            reverse=True
        )[:top_n]

        for layer, value in sorted_layers:
            print(f"Layer: {Fore.CYAN}{layer}{Style.RESET_ALL}, {metric_type.capitalize()}: {Fore.YELLOW}{value:.4f} {unit}{Style.RESET_ALL}")

    def _display_bottlenecks(self) -> None:
        """Display detected bottlenecks."""
        print(f"\n{Fore.GREEN}{Style.BRIGHT}Detected Bottlenecks:{Style.RESET_ALL}")
        for bottleneck in self.analysis_results['bottlenecks']:
            print(f"Layer: {Fore.CYAN}{bottleneck['layer']}{Style.RESET_ALL}, "
                  f"Type: {Fore.MAGENTA}{bottleneck['type']}{Style.RESET_ALL}, "
                  f"Value: {Fore.YELLOW}{bottleneck['value']:.4f}{Style.RESET_ALL}, "
                  f"Ratio: {Fore.YELLOW}{bottleneck['ratio']:.2%}{Style.RESET_ALL}")

    def _display_recommendations(self) -> None:
        """Display optimization recommendations."""
        print(f"\n{Fore.GREEN}{Style.BRIGHT}Recommendations:{Style.RESET_ALL}")
        for rec in self.analysis_results['recommendations']:
            print(f"Layer: {Fore.CYAN}{rec['layer']}{Style.RESET_ALL}")
            print(f"Recommendation: {Fore.YELLOW}{rec['recommendation']}{Style.RESET_ALL}")
            print()

    def _display_anomalies(self) -> None:
        """Display detected anomalies."""
        print(f"\n{Fore.GREEN}{Style.BRIGHT}Detected Anomalies:{Style.RESET_ALL}")
        for anomaly in self.analysis_results['anomalies']:
            print(f"Layer: {Fore.CYAN}{anomaly['layer']}{Style.RESET_ALL}, "
                  f"Type: {Fore.MAGENTA}{anomaly['type']}{Style.RESET_ALL}, "
                  f"Value: {Fore.YELLOW}{anomaly['value']:.4f}{Style.RESET_ALL}, "
                  f"Z-Score: {Fore.YELLOW}{anomaly.get('z_score', 'N/A'):.2f}{Style.RESET_ALL}")

    def _display_performance_score(self) -> None:
        """Display the overall performance score."""
        if 'performance_score' in self.analysis_results:
            score = self.analysis_results['performance_score']
            color = Fore.GREEN if score > 80 else (Fore.YELLOW if score > 60 else Fore.RED)
            print(f"\n{Fore.GREEN}{Style.BRIGHT}Overall Performance Score:{Style.RESET_ALL}")
            print(f"{color}{score:.2f}/100{Style.RESET_ALL}")

    def save_to_file(self, file_path: str) -> None:
        """Save the console report to a text file."""
        try:
            with open(file_path, 'w') as f:
                # Redirect print output to the file
                import sys
                original_stdout = sys.stdout
                sys.stdout = f
                self.display()
                sys.stdout = original_stdout
            self.logger.info(f"Console report saved to {file_path}")
        except Exception as e:
            self.logger.error(f"Error saving console report to file: {str(e)}")
Class: ConsoleReport
--------------------------------------------------------------------------------
  Method: __init__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\reporting\export_utils.py

from typing import Optional, Dict, Any
import pdfkit
import csv
import json
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

def save_report_as_pdf(html_report_path: str, pdf_output_path: str, config: Optional[Dict[str, Any]] = None) -> None:
    """
    Convert HTML report to PDF with enhanced error handling and logging.

    Args:
        html_report_path (str): Path to the HTML report file
        pdf_output_path (str): Path where the PDF report should be saved
        config (Optional[Dict[str, Any]]): Configuration options for pdfkit
    """
    options = {
        'page-size': 'A4',
        'margin-top': '0.75in',
        'margin-right': '0.75in',
        'margin-bottom': '0.75in',
        'margin-left': '0.75in',
        'encoding': "UTF-8",
        'no-outline': None,
        'enable-local-file-access': None
    }

    if config:
        options.update(config)

    try:
        pdfkit.from_file(html_report_path, pdf_output_path, options=options)
        logger.info(f"PDF report saved successfully at: {pdf_output_path}")
    except OSError as e:
        if 'wkhtmltopdf' in str(e):
            logger.error("wkhtmltopdf is not installed or not found in the system PATH.")
            logger.info("Please install wkhtmltopdf: https://wkhtmltopdf.org/downloads.html")
        else:
            logger.error(f"OS error occurred while generating PDF: {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected error occurred while generating PDF: {str(e)}")

def export_metrics_to_csv(metrics: Dict[str, Any], csv_output_path: str) -> None:
    """
    Export metrics data to a CSV file.

    Args:
        metrics (Dict[str, Any]): Metrics data to be exported
        csv_output_path (str): Path where the CSV file should be saved
    """
    try:
        with open(csv_output_path, 'w', newline='') as csvfile:
            fieldnames = ['layer', 'total_time', 'total_cpu_memory', 'total_gpu_memory', 'parameters']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

            writer.writeheader()
            for layer, data in metrics.items():
                if isinstance(data, dict):
                    writer.writerow({
                        'layer': layer,
                        'total_time': data.get('total_time', 'N/A'),
                        'total_cpu_memory': data.get('total_cpu_memory', 'N/A'),
                        'total_gpu_memory': data.get('total_gpu_memory', 'N/A'),
                        'parameters': data.get('parameters', 'N/A')
                    })
        logger.info(f"Metrics exported to CSV: {csv_output_path}")
    except Exception as e:
        logger.error(f"Error exporting metrics to CSV: {str(e)}")

def export_analysis_to_json(analysis_results: Dict[str, Any], json_output_path: str) -> None:
    """
    Export the full analysis results to a JSON file.

    Args:
        analysis_results (Dict[str, Any]): Analysis results to be exported
        json_output_path (str): Path where the JSON file should be saved
    """
    try:
        with open(json_output_path, 'w') as jsonfile:
            json.dump(analysis_results, jsonfile, indent=2)
        logger.info(f"Analysis results exported to JSON: {json_output_path}")
    except Exception as e:
        logger.error(f"Error exporting analysis results to JSON: {str(e)}")

def create_export_directory(base_path: str) -> str:
    """
    Create a directory for exporting files if it doesn't exist.

    Args:
        base_path (str): Base path for creating the export directory

    Returns:
        str: Path to the created export directory
    """
    export_dir = Path(base_path) / "memoraith_exports"
    export_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"Export directory created: {export_dir}")
    return str(export_dir)

--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\reporting\report_generator.py

import os
import asyncio
from typing import Dict, Any
import aiofiles
import logging
from jinja2 import Environment, FileSystemLoader
from ..visualization import plot_memory_usage, plot_time_usage, generate_heatmap, InteractiveDashboard
from ..config import config
from .export_utils import save_report_as_pdf, export_metrics_to_csv, export_analysis_to_json, create_export_directory

class ReportGenerator:
    """Generates comprehensive reports from the analysis results with enhanced functionality."""

    def __init__(self, analysis_results: Dict[str, Any]):
        self.analysis_results = analysis_results
        self.output_path = str(config.output_path)
        self.logger = logging.getLogger(__name__)
        self.export_dir = create_export_directory(self.output_path)

    async def generate(self, format: str = 'html') -> None:
        """
        Generate the report and save it to the output path.

        Args:
            format (str): The format of the report ('html' or 'pdf')
        """
        try:
            # Generate visualizations asynchronously
            await asyncio.gather(
                plot_memory_usage(self.analysis_results['metrics'], self.export_dir),
                plot_time_usage(self.analysis_results['metrics'], self.export_dir),
                generate_heatmap(self.analysis_results['metrics'], self.export_dir),
                InteractiveDashboard(self.analysis_results['metrics']).generate(self.export_dir)
            )

            # Generate HTML report
            html_content = self._generate_html_report()
            html_path = os.path.join(self.export_dir, 'memoraith_report.html')
            await self._save_html_report(html_content, html_path)

            # Generate PDF if requested
            if format.lower() == 'pdf':
                pdf_path = os.path.join(self.export_dir, 'memoraith_report.pdf')
                await self._save_pdf_report(html_path, pdf_path)

            # Export additional data
            await asyncio.gather(
                self._export_metrics_csv(),
                self._export_analysis_json()
            )

            self.logger.info(f"Report generation completed. Files saved in {self.export_dir}")
        except Exception as e:
            self.logger.error(f"Error during report generation: {str(e)}")
            raise

    def _generate_html_report(self) -> str:
        """Generate the HTML content for the report."""
        try:
            template_dir = os.path.join(os.path.dirname(__file__), '..', 'templates')
            env = Environment(loader=FileSystemLoader(template_dir))
            template = env.get_template('report_template.html')
            return template.render(
                analysis_results=self.analysis_results,
                config=config,
                memory_plot_path='memory_usage.png',
                time_plot_path='time_usage.png',
                heatmap_path='metrics_heatmap.png',
                dashboard_path='interactive_dashboard.html'
            )
        except Exception as e:
            self.logger.error(f"Error generating HTML report: {str(e)}")
            raise

    async def _save_html_report(self, content: str, file_path: str) -> None:
        """Save the HTML report to a file."""
        try:
            async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                await f.write(content)
            self.logger.info(f"HTML report saved to {file_path}")
        except Exception as e:
            self.logger.error(f"Error saving HTML report: {str(e)}")
            raise

    async def _save_pdf_report(self, html_path: str, pdf_path: str) -> None:
        """Save the report as a PDF file."""
        try:
            await asyncio.to_thread(save_report_as_pdf, html_path, pdf_path)
        except Exception as e:
            self.logger.error(f"Error saving PDF report: {str(e)}")
            raise

    async def _export_metrics_csv(self) -> None:
        """Export metrics to a CSV file."""
        csv_path = os.path.join(self.export_dir, 'metrics.csv')
        await asyncio.to_thread(export_metrics_to_csv, self.analysis_results['metrics'], csv_path)

    async def _export_analysis_json(self) -> None:
        """Export the full analysis results to a JSON file."""
        json_path = os.path.join(self.export_dir, 'analysis_results.json')
        await asyncio.to_thread(export_analysis_to_json, self.analysis_results, json_path)

    def get_report_files(self) -> Dict[str, str]:
        """Get a dictionary of generated report files and their paths."""
        return {
            'html_report': os.path.join(self.export_dir, 'memoraith_report.html'),
            'pdf_report': os.path.join(self.export_dir, 'memoraith_report.pdf'),
            'metrics_csv': os.path.join(self.export_dir, 'metrics.csv'),
            'analysis_json': os.path.join(self.export_dir, 'analysis_results.json'),
            'memory_plot': os.path.join(self.export_dir, 'memory_usage.png'),
            'time_plot': os.path.join(self.export_dir, 'time_usage.png'),
            'heatmap': os.path.join(self.export_dir, 'metrics_heatmap.png'),
            'interactive_dashboard': os.path.join(self.export_dir, 'interactive_dashboard.html')
        }
Class: ReportGenerator
--------------------------------------------------------------------------------
  Method: __init__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\reporting\__init__.py

from .report_generator import ReportGenerator
from .console_report import ConsoleReport
from .export_utils import save_report_as_pdf

--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\visualization\heatmap.py

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

def generate_heatmap(metrics: Dict[str, Any], output_path: str, figsize: tuple = (12, 8), cmap: str = 'viridis', annot: bool = True) -> None:
    """
    Generate an enhanced heatmap showing memory and time intensity.

    Args:
        metrics (Dict[str, Any]): Calculated metrics for each layer
        output_path (str): Path to save the generated heatmap
        figsize (tuple): Figure size (width, height) in inches
        cmap (str): Colormap for the heatmap
        annot (bool): Whether to annotate each cell with the value
    """
    try:
        data = []
        for layer, layer_metrics in metrics.items():
            if isinstance(layer_metrics, dict):
                data.append({
                    'Layer': layer,
                    'CPU Memory (MB)': layer_metrics.get('total_cpu_memory', 0),
                    'GPU Memory (MB)': layer_metrics.get('total_gpu_memory', 0),
                    'Time (s)': layer_metrics.get('total_time', 0)
                })

        df = pd.DataFrame(data)
        df.set_index('Layer', inplace=True)

        # Normalize data for better visualization
        for column in df.columns:
            df[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())

        plt.figure(figsize=figsize)
        ax = sns.heatmap(df, annot=annot, cmap=cmap, fmt='.2f',
                         linewidths=0.5, cbar_kws={'label': 'Normalized Intensity'})

        plt.title('Layer Metrics Heatmap (Normalized)', fontsize=16)
        plt.xlabel('Metrics', fontsize=12)
        plt.ylabel('Layers', fontsize=12)

        # Rotate x-axis labels for better readability
        plt.xticks(rotation=45, ha='right')

        # Adjust layout to prevent clipping of labels
        plt.tight_layout()

        # Save the figure
        plt.savefig(f"{output_path}/metrics_heatmap.png", dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Heatmap generated and saved to {output_path}/metrics_heatmap.png")
    except Exception as e:
        logger.error(f"Error generating heatmap: {str(e)}")
        raise

--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\visualization\interactive_dashboard.py

import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from typing import Dict, Any
import logging

class InteractiveDashboard:
    """
    Generates a comprehensive interactive dashboard for exploring profiling data.
    """

    def __init__(self, metrics: Dict[str, Any]):
        self.metrics = metrics
        self.logger = logging.getLogger(__name__)

    def generate(self, output_path: str) -> None:
        """
        Generate an advanced interactive dashboard and save it as an HTML file.

        Args:
            output_path (str): Path to save the generated dashboard
        """
        try:
            df = pd.DataFrame.from_dict(self.metrics, orient='index')
            df.reset_index(inplace=True)
            df.rename(columns={'index': 'Layer'}, inplace=True)

            fig = make_subplots(
                rows=3, cols=2,
                subplot_titles=(
                    "Memory Usage", "Computation Time",
                    "Parameters Count", "Layer Efficiency",
                    "Memory vs Time", "Cumulative Metrics"
                ),
                specs=[[{"type": "bar"}, {"type": "bar"}],
                       [{"type": "bar"}, {"type": "scatter"}],
                       [{"type": "scatter"}, {"type": "scatter"}]]
            )

            # Memory Usage
            fig.add_trace(go.Bar(x=df['Layer'], y=df['total_cpu_memory'], name='CPU Memory'),
                          row=1, col=1)
            fig.add_trace(go.Bar(x=df['Layer'], y=df['total_gpu_memory'], name='GPU Memory'),
                          row=1, col=1)

            # Computation Time
            fig.add_trace(go.Bar(x=df['Layer'], y=df['total_time'], name='Time'),
                          row=1, col=2)

            # Parameters Count
            fig.add_trace(go.Bar(x=df['Layer'], y=df['parameters'], name='Parameters'),
                          row=2, col=1)

            # Layer Efficiency
            efficiency = df['parameters'] / df['total_time']
            fig.add_trace(go.Scatter(x=df['Layer'], y=efficiency, mode='lines+markers', name='Efficiency'),
                          row=2, col=2)

            # Memory vs Time Scatter
            fig.add_trace(go.Scatter(x=df['total_time'], y=df['total_cpu_memory'], mode='markers', name='CPU Memory vs Time'),
                          row=3, col=1)
            fig.add_trace(go.Scatter(x=df['total_time'], y=df['total_gpu_memory'], mode='markers', name='GPU Memory vs Time'),
                          row=3, col=1)

            # Cumulative Metrics
            df_sorted = df.sort_values('total_time')
            df_sorted['cumulative_time'] = df_sorted['total_time'].cumsum()
            df_sorted['cumulative_memory'] = df_sorted['total_cpu_memory'].cumsum()
            fig.add_trace(go.Scatter(x=df_sorted['Layer'], y=df_sorted['cumulative_time'], mode='lines', name='Cumulative Time'),
                          row=3, col=2)
            fig.add_trace(go.Scatter(x=df_sorted['Layer'], y=df_sorted['cumulative_memory'], mode='lines', name='Cumulative Memory'),
                          row=3, col=2)

            fig.update_layout(height=1200, width=1600, title_text="Memoraith Advanced Profiling Results")
            fig.write_html(f"{output_path}/interactive_dashboard.html")
            self.logger.info(f"Interactive dashboard generated and saved to {output_path}/interactive_dashboard.html")
        except Exception as e:
            self.logger.error(f"Error generating interactive dashboard: {str(e)}")

    def generate_layer_comparison(self, output_path: str) -> None:
        """
        Generate a separate dashboard for layer-by-layer comparison.

        Args:
            output_path (str): Path to save the generated dashboard
        """
        try:
            df = pd.DataFrame.from_dict(self.metrics, orient='index')
            df.reset_index(inplace=True)
            df.rename(columns={'index': 'Layer'}, inplace=True)

            fig = go.Figure()

            for metric in ['total_cpu_memory', 'total_gpu_memory', 'total_time', 'parameters']:
                fig.add_trace(go.Bar(x=df['Layer'], y=df[metric], name=metric))

            fig.update_layout(
                barmode='group',
                height=600,
                width=1200,
                title_text="Layer-by-Layer Comparison",
                xaxis_title="Layers",
                yaxis_title="Metric Value (log scale)",
                yaxis_type="log"
            )

            fig.write_html(f"{output_path}/layer_comparison.html")
            self.logger.info(f"Layer comparison dashboard generated and saved to {output_path}/layer_comparison.html")
        except Exception as e:
            self.logger.error(f"Error generating layer comparison dashboard: {str(e)}")
Class: InteractiveDashboard
--------------------------------------------------------------------------------
  Method: __init__


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\visualization\plot_memory.py

import matplotlib.pyplot as plt
import numpy as np
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

def plot_memory_usage(metrics: Dict[str, Any], output_path: str, figsize: tuple = (12, 6), colors: tuple = ('skyblue', 'orange')) -> None:
    """
    Generate an enhanced bar chart for memory usage per layer.

    Args:
        metrics (Dict[str, Any]): Calculated metrics for each layer
        output_path (str): Path to save the generated plot
        figsize (tuple): Figure size (width, height) in inches
        colors (tuple): Colors for CPU and GPU memory bars
    """
    try:
        layers = []
        cpu_memory = []
        gpu_memory = []

        for layer, layer_metrics in metrics.items():
            if isinstance(layer_metrics, dict):
                layers.append(layer)
                cpu_memory.append(layer_metrics.get('total_cpu_memory', 0))
                gpu_memory.append(layer_metrics.get('total_gpu_memory', 0))

        fig, ax = plt.subplots(figsize=figsize)

        x = np.arange(len(layers))
        bar_width = 0.35

        cpu_bars = ax.bar(x - bar_width/2, cpu_memory, bar_width, label='CPU Memory', color=colors[0])
        gpu_bars = ax.bar(x + bar_width/2, gpu_memory, bar_width, label='GPU Memory', color=colors[1])

        ax.set_xlabel('Layers', fontsize=12)
        ax.set_ylabel('Memory Usage (MB)', fontsize=12)
        ax.set_title('Memory Usage per Layer', fontsize=16)
        ax.set_xticks(x)
        ax.set_xticklabels(layers, rotation=45, ha='right')
        ax.legend()

        # Add value labels on top of bars
        def autolabel(rects):
            for rect in rects:
                height = rect.get_height()
                ax.annotate(f'{height:.1f}',
                            xy=(rect.get_x() + rect.get_width() / 2, height),
                            xytext=(0, 3),  # 3 points vertical offset
                            textcoords="offset points",
                            ha='center', va='bottom', rotation=90)

        autolabel(cpu_bars)
        autolabel(gpu_bars)

        # Adjust layout to prevent clipping of labels
        plt.tight_layout()

        # Save the figure
        plt.savefig(f"{output_path}/memory_usage.png", dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Memory usage plot generated and saved to {output_path}/memory_usage.png")
    except Exception as e:
        logger.error(f"Error generating memory usage plot: {str(e)}")
        raise

--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\visualization\plot_time.py

import matplotlib.pyplot as plt
import numpy as np
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

def plot_time_usage(metrics: Dict[str, Any], output_path: str, figsize: tuple = (12, 6), color: str = 'salmon') -> None:
    """
    Generate an enhanced bar chart for computation time per layer.

    Args:
        metrics (Dict[str, Any]): Calculated metrics for each layer
        output_path (str): Path to save the generated plot
        figsize (tuple): Figure size (width, height) in inches
        color (str): Color for the time usage bars
    """
    try:
        layers = []
        times = []

        for layer, layer_metrics in metrics.items():
            if isinstance(layer_metrics, dict):
                layers.append(layer)
                times.append(layer_metrics.get('total_time', 0))

        fig, ax = plt.subplots(figsize=figsize)

        bars = ax.barh(layers, times, color=color)

        ax.set_xlabel('Computation Time (s)', fontsize=12)
        ax.set_ylabel('Layers', fontsize=12)
        ax.set_title('Computation Time per Layer', fontsize=16)

        # Add value labels at the end of bars
        for i, v in enumerate(times):
            ax.text(v, i, f' {v:.4f}s', va='center')

        # Highlight the top 3 time-consuming layers
        top_3_indices = np.argsort(times)[-3:]
        for i in top_3_indices:
            bars[i].set_color('red')
            bars[i].set_alpha(0.8)

        # Add a text box with summary statistics
        total_time = sum(times)
        avg_time = np.mean(times)
        textstr = f'Total Time: {total_time:.4f}s\nAvg Time: {avg_time:.4f}s'
        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)
        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,
                verticalalignment='top', bbox=props)

        # Adjust layout to prevent clipping of labels
        plt.tight_layout()

        # Save the figure
        plt.savefig(f"{output_path}/time_usage.png", dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Time usage plot generated and saved to {output_path}/time_usage.png")
    except Exception as e:
        logger.error(f"Error generating time usage plot: {str(e)}")
        raise

--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\visualization\real_time_visualizer.py

import logging
import numpy as np
import matplotlib.pyplot as plt
import aiofiles
from typing import Dict, Any, List
import asyncio
from ..config import config

# 2. Fix real_time_visualizer.py
class RealTimeVisualizer:
    """Real-time visualization of profiling data."""

    def __init__(self):
        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1, figsize=(10, 10))
        self.memory_data = {}
        self.time_data = {}
        self.animation = None
        plt.ion()  # Turn on interactive mode
        self.fig.show()

    async def update(self, data: Dict[str, Any]) -> None:
        """Update the visualization with new data."""
        for layer, layer_data in data.items():
            self.memory_data[layer] = layer_data.get('cpu_memory', 0)
            self.time_data[layer] = layer_data.get('time', 0)

        await self._draw()

    async def _draw(self) -> None:
        """Draw the updated visualization."""
        self.ax1.clear()
        self.ax2.clear()

        layers = list(self.memory_data.keys())
        memory_values = list(self.memory_data.values())
        time_values = list(self.time_data.values())

        self.ax1.barh(layers, memory_values)
        self.ax1.set_xlabel('Memory Usage (MB)')
        self.ax1.set_title('Real-time Memory Usage by Layer')

        self.ax2.barh(layers, time_values)
        self.ax2.set_xlabel('Computation Time (s)')
        self.ax2.set_title('Real-time Computation Time by Layer')

        plt.tight_layout()
        await asyncio.to_thread(self.fig.canvas.draw)
        await asyncio.to_thread(self.fig.canvas.flush_events)

    def close(self):
        """Close the visualization window."""
        plt.close(self.fig)

    async def add_data(self, memory: float, time: float) -> None:
        """Add new data points to the visualization."""
        self.memory_data = memory
        self.time_data = time
        await self._draw()

    async def stop(self) -> None:
        """Stop the visualization."""
        if self.animation:
            self.animation.event_source.stop()
        plt.close(self.fig)

# 3. Fix TensorFlowAdapter _wrapped_call method
async def _wrapped_call(self, *args, **kwargs) -> Any:
    """Wrapped call method for profiling each layer."""
    output = None
    for layer in self.model.layers:
        layer_name = f"{layer.__class__.__name__}_{id(layer)}"
        self.time_tracker.start(layer_name)
        output = layer(*args, **kwargs)
        self.time_tracker.stop(layer_name)

        try:
            self.data[layer_name] = self.data.get(layer_name, {})
            self.data[layer_name]['time'] = self.time_tracker.get_duration(layer_name)
            self.data[layer_name]['parameters'] = layer.count_params()

            if config.enable_memory:
                self.data[layer_name]['cpu_memory'] = await self.cpu_tracker.get_peak_memory()
                if self.gpu_tracker:
                    self.data[layer_name]['gpu_memory'] = await self.gpu_tracker.get_peak_memory()

            if hasattr(output, 'shape'):
                self.data[layer_name]['output_shape'] = output.shape.as_list()
        except Exception as e:
            self.logger.error(f"Error in _wrapped_call for layer {layer_name}: {str(e)}")

        args = (output,)

    return output
Class: RealTimeVisualizer
--------------------------------------------------------------------------------
  Method: __init__
  Method: close


--------------------------------------------------------------------------------

File: C:\Users\PC\Desktop\Leo-Major\Memoraith\memoraith\visualization\__init__.py

from .plot_memory import plot_memory_usage
from .plot_time import plot_time_usage
from .heatmap import generate_heatmap
from .interactive_dashboard import InteractiveDashboard
from .real_time_visualizer import RealTimeVisualizer


def config():
    return None

--------------------------------------------------------------------------------

